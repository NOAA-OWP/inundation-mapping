{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "544d20bf-6273-4dd5-bae8-a42935b3e3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tools_shared_functions import (\n",
    "    aggregate_wbd_hucs,\n",
    "    filter_nwm_segments_by_stream_order,\n",
    "    flow_data,\n",
    "    get_metadata,\n",
    "    get_nwm_segs,\n",
    "    get_thresholds,\n",
    ")\n",
    "\n",
    "import utils.fim_logger as fl\n",
    "from utils.shared_variables import VIZ_PROJECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c7d9207-9b2b-431b-ab4e-3dbb3e2107f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Functions to get process and filter the metadata\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_of_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list = list_of_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "        lid_list.append(nws_lid)\n",
    "    if verbose == True:\n",
    "        print(f'List of LIDs: {lid_list}')\n",
    "        \n",
    "    return lid_list\n",
    "\n",
    "# lid_list = list_of_lids(conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_duplicate_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of duplicate LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    duplicate_lid_list = []\n",
    "     \n",
    "    \n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid in lid_list:\n",
    "            duplicate_lid_list.append(nws_lid)\n",
    "        else: \n",
    "            lid_list.append(nws_lid)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Length of unique LID list: {len(lid_list)}')\n",
    "        print(f'List of duplicate LIDs: {duplicate_lid_list}')\n",
    "\n",
    "        \n",
    "    return lid_list, duplicate_lid_list\n",
    "\n",
    "# lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_lid(lid_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by LID\n",
    "    \n",
    "    Example:\n",
    "    conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        lid = site['identifiers']['nws_lid']\n",
    "        if lid == lid_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'LID filter: {lid_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_state(state_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by state\n",
    "    \n",
    "    Example: \n",
    "    conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        state = site['nws_data']['state']\n",
    "        if state == state_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'State: {state_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dca1b28-976d-4487-9fd3-96253e1e83f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Testing get_metadata() functionality\n",
    "\n",
    "\n",
    "# --------- Inputs --------- \n",
    "\n",
    "\n",
    "search = 5\n",
    "\n",
    "nwm_us_search, nwm_ds_search = search, search\n",
    "\n",
    "\n",
    "# output_catfim_dir = \n",
    "API_BASE_URL = 'https://nwcal-wrds.nwc.nws.noaa.gov/api/location/v3.0'\n",
    "metadata_url = f'{API_BASE_URL}/metadata'\n",
    "\n",
    "\n",
    "# lid_to_run = \n",
    "# nwm_metafile = \n",
    "\n",
    "# --------- Code --------- \n",
    "\n",
    "all_meta_lists = []\n",
    "\n",
    "\n",
    "conus_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='nws_lid',\n",
    "    selector=['all'],\n",
    "    must_include='nws_data.rfc_forecast_point',\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "\n",
    "\n",
    "# Get metadata for Islands and Alaska\n",
    "islands_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='state',\n",
    "    selector=['HI', 'PR', 'AK'],\n",
    "    must_include=None,\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "# Append the lists\n",
    "all_meta_lists = conus_list + islands_list\n",
    "\n",
    "# print(islands_list)\n",
    "\n",
    "# with open(meta_file, \"wb\") as p_handle:\n",
    "#     pickle.dump(all_meta_lists, p_handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a27ec31-f7d3-4fba-943b-362f848813fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'Length of conus_list: {len(conus_list)}')\n",
    "# print(f'Length of islands_list: {len(islands_list)}')\n",
    "# print(f'Length of all_meta_lists: {len(all_meta_lists)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25a4979-2b0c-4f4f-ae41-6fec3768d39b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input metadata list length: 7631\n",
      "Output (unique) metadata list length: 7214\n",
      "Number of unique LIDs: 7214 \n",
      "Number of duplicate LIDs: 152 \n",
      "Number of None LIDs: 265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ New addition: filtering ------\n",
    "\n",
    "\n",
    "\n",
    "# -- function --\n",
    "def filter_metadata_list (metadata_list, verbose):\n",
    "    '''\n",
    "    \n",
    "    Filter metadata list to remove: \n",
    "    - sites where the nws_lid = None\n",
    "    - duplicate sites\n",
    "    \n",
    "    '''\n",
    "\n",
    "    unique_lids, duplicate_lids = [], []\n",
    "    duplicate_metadata_list, unique_metadata_list = [], []\n",
    "\n",
    "    nonelid_metadata_list = [] # TODO: remove eventually?    \n",
    "\n",
    "    for i, site in enumerate(metadata_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid == None:\n",
    "            # No LID available\n",
    "            nonelid_metadata_list.append(site)\n",
    "\n",
    "            # TODO: replace this with Continue, eventually we wont need this list\n",
    "\n",
    "        elif nws_lid in unique_lids:\n",
    "            # Duplicate LID\n",
    "            duplicate_lids.append(nws_lid)\n",
    "            duplicate_metadata_list.append(site)\n",
    "\n",
    "        else: \n",
    "            # Unique/unseen LID that's not None\n",
    "            unique_lids.append(nws_lid)\n",
    "            unique_metadata_list.append(site)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Input metadata list length: {len(metadata_list)}')\n",
    "        print(f'Output (unique) metadata list length: {len(unique_metadata_list)}')\n",
    "        print(f'Number of unique LIDs: {len(unique_lids)} \\nNumber of duplicate LIDs: {len(duplicate_lids)} \\nNumber of None LIDs: {len(nonelid_metadata_list)}')\n",
    "\n",
    "    return unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list # TODO: eventually, have it only return necessary objects\n",
    "\n",
    "\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(all_meta_lists, True)\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffc3b60-18de-4825-8766-80f3904fd6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Code: Single API call (only forecast points)\n",
      "State: Puerto Rico \n",
      "Number of sites: 5\n",
      "\n",
      "State: Hawaii \n",
      "Number of sites: 2\n",
      "\n",
      "State: Alaska \n",
      "Number of sites: 145\n",
      "\n",
      "\n",
      "Proposed Update: Double API call (forecast points + all HI, AK, and PR points)\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Puerto Rico \n",
      "Number of sites: 238\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Hawaii \n",
      "Number of sites: 495\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Alaska \n",
      "Number of sites: 1950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get state counts\n",
    "\n",
    "state_list = ['Puerto Rico', 'Hawaii', 'Alaska']\n",
    "\n",
    "print('Current Code: Single API call (only forecast points)')\n",
    "for state in state_list: \n",
    "    currentcode_state = filter_by_state(state, conus_list, True)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('Proposed Update: Double API call (forecast points + all HI, AK, and PR points)')\n",
    "print()\n",
    "for state in state_list: \n",
    "    # print('Before filtering out duplicates:')\n",
    "    # prefilt_state = filter_by_state(state, all_meta_lists, True)\n",
    "    print('AFTER filtering out duplicates:')\n",
    "    postfilt_state = filter_by_state(state, unique_metadata_list, True)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2ad926-105c-4326-980c-3e4793b7b58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Connecticut \n",
      "Number of sites: 23\n",
      "State: New York \n",
      "Number of sites: 142\n",
      "State: Texas \n",
      "Number of sites: 380\n"
     ]
    }
   ],
   "source": [
    "postfilt_state = filter_by_state('Connecticut', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('New York', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('Texas', unique_metadata_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65996d68-b060-4cd3-b575-3e3d4172c532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input metadata list length: 4679\n",
      "Output (unique) metadata list length: 4679\n",
      "Number of unique LIDs: 4679 \n",
      "Number of duplicate LIDs: 0 \n",
      "Number of None LIDs: 0\n",
      "\n",
      "State: Alaska \n",
      "Number of sites: 145\n"
     ]
    }
   ],
   "source": [
    "## Current code formulation\n",
    "\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(conus_list, True)\n",
    "print()\n",
    "conus_list_filt = filter_by_state('Alaska', conus_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd1a64b-35c3-4f75-9e08-e936c17da1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LID filter: None \n",
      "Number of sites: 265\n"
     ]
    }
   ],
   "source": [
    "# lid_list, duplicate_lid_list = list_duplicate_lids(all_meta_lists, True)\n",
    "\n",
    "conus_list_filt = filter_by_lid(None, islands_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bee81b-f67f-48bf-8dd9-86f60dd3e6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352ca99d-3b29-4ae9-848c-e2f55f9bebb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Trying to time the API call (but the results were weird)\n",
    "# from time import process_time\n",
    "# t1_start = process_time() # stopwatch\n",
    " \n",
    "\n",
    "# conus_list, ___ = get_metadata(\n",
    "#     metadata_url,\n",
    "#     select_by='nws_lid',\n",
    "#     selector=['all'],\n",
    "#     must_include='nws_data.rfc_forecast_point',\n",
    "#     upstream_trace_distance=nwm_us_search,\n",
    "#     downstream_trace_distance=nwm_ds_search,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# print('List length:', len(conus_list))\n",
    "\n",
    "# t1_stop = process_time() # stopwatch\n",
    "# print('Runtime (seconds):', t1_stop-t1_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddc9047-5aa3-4459-ad12-3c9553850a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conus_list[1]\n",
    "# conus_list[1]['identifiers']['nws_lid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ec188992-fda6-4f8b-876f-be78c48c67cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# state_list, ___ = get_metadata(\n",
    "#     metadata_url,\n",
    "#     select_by='state',\n",
    "#     # selector=['HI', 'PR', 'AK'],\n",
    "#     selector=['AK'],\n",
    "\n",
    "#     # must_include='identifiers.nws_lid', ## ddin't work oh well\n",
    "#     must_include=None,\n",
    "\n",
    "#     # must_include='nws_data.rfc_forecast_point',\n",
    "#     upstream_trace_distance=nwm_us_search,\n",
    "#     downstream_trace_distance=nwm_ds_search,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea62763-40ca-4284-a7cd-1f6c6a558b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(state_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48ebeb97-42ef-4021-b777-083cf6267e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filt_list = conus_list[conus_list['identifiers']['nws_lid']=='00RDR']\n",
    "# type(conus_list)\n",
    "# list of dictionaries of dictionaries \n",
    "# type(conus_list[1])\n",
    "# conus_list[1].keys()\n",
    "# conus_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba42000-3855-47eb-9f1c-f3c805f45b96",
   "metadata": {},
   "source": [
    "### Get a HUC list for a given HUC02 region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "034acbe3-d096-4dfd-a358-4ca409bfef64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19020101\n",
      "19020102\n",
      "19020103\n",
      "19020104\n",
      "19020201\n",
      "19020202\n",
      "19020203\n",
      "19020301\n",
      "19020302\n",
      "19020401\n",
      "19020402\n",
      "19020501\n",
      "19020502\n",
      "19020503\n",
      "19020504\n",
      "19020505\n",
      "19020601\n",
      "19020602\n",
      "19020800\n"
     ]
    }
   ],
   "source": [
    "fim_output_path = '/data/previous_fim/fim_4_5_2_11/'\n",
    "\n",
    "# huc2 = '20' # Hawaii\n",
    "# huc2 = '21' # Puerto Rico\n",
    "huc2 = '19' # Alaska\n",
    "\n",
    "\n",
    "all_hucs = os.listdir(fim_output_path)\n",
    "\n",
    "subsetted_hucs = [x for x in all_hucs if x.startswith(huc2)]\n",
    "\n",
    "\n",
    "for i in subsetted_hucs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a52847-3189-41b7-a755-3063512a96e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get stats for current CatFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ca99d060-3963-497f-a8c0-cb117bbcd818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Inputs\n",
    "states = ['AK', 'PR', 'HI']\n",
    "\n",
    "## Previous runs\n",
    "catfim_folder_prev = '/data/catfim/'\n",
    "result_folders_prev = ['hand_4_5_11_1_flow_based', 'hand_4_5_11_1_stage_based', 'fim_4_5_2_11_flow_based', 'fim_4_5_2_11_stage_based']\n",
    "\n",
    "## Current test runs\n",
    "catfim_folder_testing = '/data/catfim/emily_test'\n",
    "result_folders_testing = ['site_filtering_HI_flow_based', 'site_filtering_HI_stage_based', \n",
    "                  'site_filtering_PR_flow_based', 'site_filtering_PR_stage_based', \n",
    "                  'site_filtering_AK_flow_based', 'site_filtering_AK_stage_based']\n",
    "\n",
    "\n",
    "\n",
    "def count_mapped_for_state(catfim_folder, result_folders, states):\n",
    "    # Read in CatFIM outputs\n",
    "    for result_folder in result_folders:\n",
    "        print()\n",
    "        print('-----' + result_folder + '-----')\n",
    "        \n",
    "        catfim_points_path = 'None'\n",
    "        \n",
    "        catfim_outputs_mapping_path = os.path.join(catfim_folder, result_folder, 'mapping')\n",
    "            \n",
    "        # Get filepath\n",
    "        for file in os.listdir(catfim_outputs_mapping_path):\n",
    "            if file.endswith('catfim_sites.csv'):\n",
    "                catfim_points_path = os.path.join(catfim_outputs_mapping_path, file)\n",
    "\n",
    "        if catfim_points_path == 'None':\n",
    "            print(f'No site csv found in {catfim_outputs_mapping_path}')\n",
    "            continue\n",
    "        \n",
    "        # Open points file\n",
    "        try:\n",
    "            catfim_points = gpd.read_file(catfim_points_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('An error occurred', e)\n",
    "            continue\n",
    "\n",
    "        # Get mapped vs unmapped data for the listed states\n",
    "        for state in states:\n",
    "            catfim_points_state = catfim_points[catfim_points['states'] == state]\n",
    "            \n",
    "            if len(catfim_points_state) != 0:\n",
    "                num_not_mapped = len(catfim_points_state[catfim_points_state['mapped'] == 'no'])\n",
    "                num_mapped = len(catfim_points_state[catfim_points_state['mapped'] == 'yes'])\n",
    "                \n",
    "                huc_list = catfim_points_state['HUC8']\n",
    "                \n",
    "                \n",
    "                if 'ahps_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['ahps_lid']\n",
    "                elif 'nws_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['nws_lid']\n",
    "                else:\n",
    "                    print('Could not find ahps_lid or nws_lid column in csv.')                   \n",
    "                    print(catfim_points_state.columns)\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                huc_list_unique = set(huc_list)\n",
    "                lid_list_unique = set(lid_list)\n",
    "                num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "\n",
    "                print(f'{state} \\n   Mapped: {num_mapped} \\n   Not mapped: {num_not_mapped}')\n",
    "                \n",
    "                print(f'   Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "                print(f'   {len(huc_list_unique)} hucs: {huc_list_unique}')\n",
    "                \n",
    "        # return catfim_points, catfim_points_state\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6255a914-3c44-4e02-8c20-a6f11eb5a12e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----hand_4_5_11_1_flow_based-----\n",
      "AK \n",
      "   Mapped: 14 \n",
      "   Not mapped: 39\n",
      "   Number of duplicate LIDs: 0\n",
      "   14 hucs: {'19020503', '19020202', '19020302', '19020502', '19020402', '19020401', '19020104', '19020101', '19020102', '19020505', '19020504', '19020501', '19020201', '19020301'}\n",
      "PR \n",
      "   Mapped: 4 \n",
      "   Not mapped: 1\n",
      "   Number of duplicate LIDs: 0\n",
      "   2 hucs: {'21010005', '21010002'}\n",
      "HI \n",
      "   Mapped: 1 \n",
      "   Not mapped: 1\n",
      "   Number of duplicate LIDs: 0\n",
      "   2 hucs: {'20020000', '20010000'}\n",
      "\n",
      "-----hand_4_5_11_1_stage_based-----\n",
      "AK \n",
      "   Mapped: 13 \n",
      "   Not mapped: 40\n",
      "   Number of duplicate LIDs: 0\n",
      "   14 hucs: {'19020503', '19020202', '19020302', '19020502', '19020402', '19020401', '19020104', '19020101', '19020102', '19020505', '19020504', '19020501', '19020201', '19020301'}\n",
      "PR \n",
      "   Mapped: 0 \n",
      "   Not mapped: 5\n",
      "   Number of duplicate LIDs: 0\n",
      "   2 hucs: {'21010005', '21010002'}\n",
      "HI \n",
      "   Mapped: 0 \n",
      "   Not mapped: 2\n",
      "   Number of duplicate LIDs: 0\n",
      "   2 hucs: {'20020000', '20010000'}\n",
      "\n",
      "-----fim_4_5_2_11_flow_based-----\n",
      "PR \n",
      "   Mapped: 68 \n",
      "   Not mapped: 177\n",
      "   Number of duplicate LIDs: 5\n",
      "   5 hucs: {'21010005', '21010003', '21010002', '21010004', '21010008'}\n",
      "HI \n",
      "   Mapped: 50 \n",
      "   Not mapped: 441\n",
      "   Number of duplicate LIDs: 2\n",
      "   7 hucs: {'20050000', '20060000', '20030000', '20020000', '20010000', '20070000', '20040000'}\n",
      "\n",
      "-----fim_4_5_2_11_stage_based-----\n",
      "PR \n",
      "   Mapped: 0 \n",
      "   Not mapped: 245\n",
      "   Number of duplicate LIDs: 5\n",
      "   5 hucs: {'21010005', '21010003', '21010002', '21010004', '21010008'}\n",
      "HI \n",
      "   Mapped: 0 \n",
      "   Not mapped: 491\n",
      "   Number of duplicate LIDs: 2\n",
      "   7 hucs: {'20050000', '20060000', '20030000', '20020000', '20010000', '20070000', '20040000'}\n"
     ]
    }
   ],
   "source": [
    "count_mapped_for_state(catfim_folder_prev, result_folders_prev, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f45696-893c-4709-9fe6-d1c223bd9020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_mapped_for_state(catfim_folder_testing, result_folders_testing, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "585a3963-36fd-4983-8f03-4042b566c7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "Index(['field_1', 'wrds_timestamp', 'nrldb_timestamp', 'nwis_timestamp',\n",
      "       'metadata_sources', 'ahps_lid', 'usgs_gage', 'nwm_seg',\n",
      "       'identifiers_goes_id', 'identifiers_env_can_gage_id', 'nws_data_name',\n",
      "       'nws_data_wfo', 'nws_data_rfc', 'nws_data_geo_rfc', 'nws_data_latitude',\n",
      "       'nws_data_longitude', 'nws_data_map_link',\n",
      "       'nws_data_horizontal_datum_name', 'nws_data_state', 'nws_data_county',\n",
      "       'nws_data_county_code', 'nws_data_huc', 'nws_data_hsa',\n",
      "       'nws_data_zero_datum', 'nws_data_vertical_datum_name',\n",
      "       'nws_data_rfc_forecast_point', 'nws_data_rfc_defined_fcst_point',\n",
      "       'nws_data_riverpoint', 'usgs_data_name', 'usgs_data_geo_rfc',\n",
      "       'usgs_data_latitude', 'usgs_data_longitude', 'usgs_data_map_link',\n",
      "       'usgs_data_coord_accuracy_code', 'usgs_data_latlon_datum_name',\n",
      "       'usgs_data_coord_method_code', 'usgs_data_state', 'usgs_data_huc',\n",
      "       'usgs_data_site_type', 'usgs_data_altitude',\n",
      "       'usgs_data_alt_accuracy_code', 'usgs_data_alt_datum_code',\n",
      "       'usgs_data_alt_method_code', 'usgs_data_drainage_area',\n",
      "       'usgs_data_drainage_area_units', 'usgs_data_contrib_drainage_area',\n",
      "       'usgs_data_active', 'usgs_data_gages_ii_reference',\n",
      "       'nwm_feature_data_downstream_feature_id', 'nwm_feature_data_latitude',\n",
      "       'nwm_feature_data_longitude', 'nwm_feature_data_altitude',\n",
      "       'nwm_feature_data_stream_length', 'nwm_feature_data_stream_order',\n",
      "       'nwm_feature_data_mannings_roughness', 'nwm_feature_data_slope',\n",
      "       'nwm_feature_data_channel_side_slope',\n",
      "       'nwm_feature_data_nhd_waterbody_comid', 'env_can_gage_data_name',\n",
      "       'env_can_gage_data_latitude', 'env_can_gage_data_longitude',\n",
      "       'env_can_gage_data_map_link', 'env_can_gage_data_drainage_area',\n",
      "       'env_can_gage_data_contrib_drainage_area',\n",
      "       'env_can_gage_data_water_course', 'nws_preferred_name',\n",
      "       'nws_preferred_latitude', 'nws_preferred_longitude',\n",
      "       'nws_preferred_latlon_datum_name', 'nws_preferred_state',\n",
      "       'nws_preferred_huc', 'usgs_preferred_name', 'usgs_preferred_latitude',\n",
      "       'usgs_preferred_longitude', 'usgs_preferred_latlon_datum_name',\n",
      "       'usgs_preferred_state', 'usgs_preferred_huc',\n",
      "       'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id',\n",
      "       'crosswalk_datasets_location_nwm_crosswalk_dataset_name',\n",
      "       'crosswalk_datasets_location_nwm_crosswalk_dataset_description',\n",
      "       'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id',\n",
      "       'crosswalk_datasets_nws_usgs_crosswalk_dataset_name',\n",
      "       'crosswalk_datasets_nws_usgs_crosswalk_dataset_description',\n",
      "       'assigned_crs', 'HUC8', 'name', 'states', 'mapped', 'status',\n",
      "       'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "04988eab-2866-4d5b-882a-abb76e072df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate LIDs: 0\n",
      "7 hucs: {'20050000', '20060000', '20030000', '20020000', '20010000', '20070000', '20040000'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "huc_list = catfim_points_state['HUC8']\n",
    "lid_list = catfim_points_state['ahps_lid']\n",
    "\n",
    "\n",
    "huc_list_unique = set(huc_list)\n",
    "lid_list_unique = set(lid_list)\n",
    "num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "\n",
    "\n",
    "print(f'Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "print(f'{len(huc_list_unique)} hucs: {huc_list_unique}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "945bb5ff-fa5d-42d4-adfa-fed10bb2076a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_1</th>\n",
       "      <th>wrds_timestamp</th>\n",
       "      <th>nrldb_timestamp</th>\n",
       "      <th>nwis_timestamp</th>\n",
       "      <th>metadata_sources</th>\n",
       "      <th>ahps_lid</th>\n",
       "      <th>usgs_gage</th>\n",
       "      <th>nwm_seg</th>\n",
       "      <th>identifiers_goes_id</th>\n",
       "      <th>identifiers_env_can_gage_id</th>\n",
       "      <th>...</th>\n",
       "      <th>crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id</th>\n",
       "      <th>crosswalk_datasets_nws_usgs_crosswalk_dataset_name</th>\n",
       "      <th>crosswalk_datasets_nws_usgs_crosswalk_dataset_description</th>\n",
       "      <th>assigned_crs</th>\n",
       "      <th>HUC8</th>\n",
       "      <th>name</th>\n",
       "      <th>states</th>\n",
       "      <th>mapped</th>\n",
       "      <th>status</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>nlih1</td>\n",
       "      <td>16717000</td>\n",
       "      <td>800008995</td>\n",
       "      <td>D114953C</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20010000</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>HI</td>\n",
       "      <td>yes</td>\n",
       "      <td>Missing flow data for action; moderate; major;...</td>\n",
       "      <td>POINT (-17271423.078598868 2245133.7728782427)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>wwkh1</td>\n",
       "      <td>16518000</td>\n",
       "      <td>800015240</td>\n",
       "      <td>DD5E710A</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20020000</td>\n",
       "      <td>Maui</td>\n",
       "      <td>HI</td>\n",
       "      <td>yes</td>\n",
       "      <td>Missing flow data for action; moderate; record</td>\n",
       "      <td>POINT (-17381756.156252272 2369756.847743014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>aeph1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20070000</td>\n",
       "      <td>Kauai</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17754685.729507785 2501056.3808664195)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>ahmh1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20010000</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17322766.10523033 2253658.9837820856)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>ahuh1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20060000</td>\n",
       "      <td>Oahu</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17570359.20600229 2443458.5061386973)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>484</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>ito</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20010000</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17260732.700543668 2240150.578406945)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>lih</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20070000</td>\n",
       "      <td>Kauai</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17737502.947662175 2509184.187952809)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>486</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>ogg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20020000</td>\n",
       "      <td>Maui</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17414375.861736543 2378612.189414868)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>487</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>phnl</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20060000</td>\n",
       "      <td>Oahu</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17581738.531727582 2430541.04639686)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>488</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>NWS data: NRLDB - Last updated: 2024-12-10 22:...</td>\n",
       "      <td>USGS data: USGS NWIS - Last updated: 2024-12-1...</td>\n",
       "      <td>['NWS data: NRLDB - Last updated: 2024-12-10 2...</td>\n",
       "      <td>phog</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NWS Station to USGS Gages 3.0</td>\n",
       "      <td>Created 20240328.  Authoritative 3.0 dataset m...</td>\n",
       "      <td>EPSG:4269 Assumed</td>\n",
       "      <td>20020000</td>\n",
       "      <td>Maui</td>\n",
       "      <td>HI</td>\n",
       "      <td>no</td>\n",
       "      <td>Error getting stages values from WRDS API</td>\n",
       "      <td>POINT (-17414375.861736543 2378612.189414868)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    field_1                                     wrds_timestamp  \\\n",
       "0         0  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "1         1  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "2         2  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "3         3  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "4         4  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "..      ...                                                ...   \n",
       "484     484  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "485     485  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "486     486  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "487     487  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "488     488  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "\n",
       "                                       nrldb_timestamp  \\\n",
       "0    NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "1    NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "2    NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "3    NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "4    NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "..                                                 ...   \n",
       "484  NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "485  NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "486  NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "487  NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "488  NWS data: NRLDB - Last updated: 2024-12-10 22:...   \n",
       "\n",
       "                                        nwis_timestamp  \\\n",
       "0    USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "1    USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "2    USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "3    USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "4    USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "..                                                 ...   \n",
       "484  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "485  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "486  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "487  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "488  USGS data: USGS NWIS - Last updated: 2024-12-1...   \n",
       "\n",
       "                                      metadata_sources ahps_lid usgs_gage  \\\n",
       "0    ['NWS data: NRLDB - Last updated: 2024-12-10 2...    nlih1  16717000   \n",
       "1    ['NWS data: NRLDB - Last updated: 2024-12-10 2...    wwkh1  16518000   \n",
       "2    ['NWS data: NRLDB - Last updated: 2024-12-10 2...    aeph1             \n",
       "3    ['NWS data: NRLDB - Last updated: 2024-12-10 2...    ahmh1             \n",
       "4    ['NWS data: NRLDB - Last updated: 2024-12-10 2...    ahuh1             \n",
       "..                                                 ...      ...       ...   \n",
       "484  ['NWS data: NRLDB - Last updated: 2024-12-10 2...      ito             \n",
       "485  ['NWS data: NRLDB - Last updated: 2024-12-10 2...      lih             \n",
       "486  ['NWS data: NRLDB - Last updated: 2024-12-10 2...      ogg             \n",
       "487  ['NWS data: NRLDB - Last updated: 2024-12-10 2...     phnl             \n",
       "488  ['NWS data: NRLDB - Last updated: 2024-12-10 2...     phog             \n",
       "\n",
       "       nwm_seg identifiers_goes_id identifiers_env_can_gage_id  ...  \\\n",
       "0    800008995            D114953C                              ...   \n",
       "1    800015240            DD5E710A                              ...   \n",
       "2                                                               ...   \n",
       "3                                                               ...   \n",
       "4                                                               ...   \n",
       "..         ...                 ...                         ...  ...   \n",
       "484                                                             ...   \n",
       "485                                                             ...   \n",
       "486                                                             ...   \n",
       "487                                                             ...   \n",
       "488                                                             ...   \n",
       "\n",
       "    crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id  \\\n",
       "0                                                  3.0                            \n",
       "1                                                  3.0                            \n",
       "2                                                  3.0                            \n",
       "3                                                  3.0                            \n",
       "4                                                  3.0                            \n",
       "..                                                 ...                            \n",
       "484                                                3.0                            \n",
       "485                                                3.0                            \n",
       "486                                                3.0                            \n",
       "487                                                3.0                            \n",
       "488                                                3.0                            \n",
       "\n",
       "    crosswalk_datasets_nws_usgs_crosswalk_dataset_name  \\\n",
       "0                        NWS Station to USGS Gages 3.0   \n",
       "1                        NWS Station to USGS Gages 3.0   \n",
       "2                        NWS Station to USGS Gages 3.0   \n",
       "3                        NWS Station to USGS Gages 3.0   \n",
       "4                        NWS Station to USGS Gages 3.0   \n",
       "..                                                 ...   \n",
       "484                      NWS Station to USGS Gages 3.0   \n",
       "485                      NWS Station to USGS Gages 3.0   \n",
       "486                      NWS Station to USGS Gages 3.0   \n",
       "487                      NWS Station to USGS Gages 3.0   \n",
       "488                      NWS Station to USGS Gages 3.0   \n",
       "\n",
       "    crosswalk_datasets_nws_usgs_crosswalk_dataset_description  \\\n",
       "0    Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "1    Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "2    Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "3    Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "4    Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "..                                                 ...          \n",
       "484  Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "485  Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "486  Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "487  Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "488  Created 20240328.  Authoritative 3.0 dataset m...          \n",
       "\n",
       "          assigned_crs      HUC8    name states mapped  \\\n",
       "0    EPSG:4269 Assumed  20010000  Hawaii     HI    yes   \n",
       "1    EPSG:4269 Assumed  20020000    Maui     HI    yes   \n",
       "2    EPSG:4269 Assumed  20070000   Kauai     HI     no   \n",
       "3    EPSG:4269 Assumed  20010000  Hawaii     HI     no   \n",
       "4    EPSG:4269 Assumed  20060000    Oahu     HI     no   \n",
       "..                 ...       ...     ...    ...    ...   \n",
       "484  EPSG:4269 Assumed  20010000  Hawaii     HI     no   \n",
       "485  EPSG:4269 Assumed  20070000   Kauai     HI     no   \n",
       "486  EPSG:4269 Assumed  20020000    Maui     HI     no   \n",
       "487  EPSG:4269 Assumed  20060000    Oahu     HI     no   \n",
       "488  EPSG:4269 Assumed  20020000    Maui     HI     no   \n",
       "\n",
       "                                                status  \\\n",
       "0    Missing flow data for action; moderate; major;...   \n",
       "1       Missing flow data for action; moderate; record   \n",
       "2            Error getting stages values from WRDS API   \n",
       "3            Error getting stages values from WRDS API   \n",
       "4            Error getting stages values from WRDS API   \n",
       "..                                                 ...   \n",
       "484          Error getting stages values from WRDS API   \n",
       "485          Error getting stages values from WRDS API   \n",
       "486          Error getting stages values from WRDS API   \n",
       "487          Error getting stages values from WRDS API   \n",
       "488          Error getting stages values from WRDS API   \n",
       "\n",
       "                                           geometry  \n",
       "0    POINT (-17271423.078598868 2245133.7728782427)  \n",
       "1     POINT (-17381756.156252272 2369756.847743014)  \n",
       "2    POINT (-17754685.729507785 2501056.3808664195)  \n",
       "3     POINT (-17322766.10523033 2253658.9837820856)  \n",
       "4     POINT (-17570359.20600229 2443458.5061386973)  \n",
       "..                                              ...  \n",
       "484   POINT (-17260732.700543668 2240150.578406945)  \n",
       "485   POINT (-17737502.947662175 2509184.187952809)  \n",
       "486   POINT (-17414375.861736543 2378612.189414868)  \n",
       "487    POINT (-17581738.531727582 2430541.04639686)  \n",
       "488   POINT (-17414375.861736543 2378612.189414868)  \n",
       "\n",
       "[489 rows x 90 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catfim_points_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b5d0836-5596-4ac4-a0a1-98bdbe7cbd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unique LID list: 489\n",
      "List of duplicate LIDs: []\n"
     ]
    }
   ],
   "source": [
    "# print(len(catfim_points[catfim_points['mapped']=='yes']))\n",
    "\n",
    "# mapped_points_state = catfim_points_state[catfim_points_state['mapped']=='yes']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_points = catfim_points_state\n",
    "\n",
    "\n",
    "# ---- function ----\n",
    "\n",
    "lid_list = []\n",
    "duplicate_lid_list = []\n",
    "\n",
    "\n",
    "for index, row in input_points.iterrows():\n",
    "    nws_lid = row['ahps_lid']\n",
    "\n",
    "    if nws_lid in lid_list:\n",
    "        duplicate_lid_list.append(nws_lid)\n",
    "    else: \n",
    "        lid_list.append(nws_lid)\n",
    "\n",
    "print(f'Length of unique LID list: {len(lid_list)}')\n",
    "print(f'List of duplicate LIDs: {duplicate_lid_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd9a09cc-3a5e-4c9b-a419-ba008f999889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mapped_points_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae429a31-f83e-4594-9245-66cfb452b2be",
   "metadata": {},
   "source": [
    "### Test state column instablility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cce08d89-cf96-4905-a090-3ba3a09695b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "input_meta_list = conus_list # conus_list or islands_list\n",
    "\n",
    "# -----\n",
    "\n",
    "state_data = []\n",
    "\n",
    "for i, site in enumerate(input_meta_list):\n",
    "    lid = site['identifiers']['nws_lid']\n",
    "\n",
    "    nws_data_state = site['nws_data']['state']\n",
    "    usgs_data_state = site['usgs_data']['state']\n",
    "    nws_preferred_state = site['nws_preferred']['state']\n",
    "    usgs_preferred_state = site['usgs_preferred']['state']\n",
    "\n",
    "    row = {'index': i, 'lid': lid, \n",
    "           'nws_data_state':nws_data_state,\n",
    "           'usgs_data_state':usgs_data_state,\n",
    "           'nws_preferred_state':nws_preferred_state,\n",
    "           'usgs_preferred_state':usgs_preferred_state}\n",
    "\n",
    "    state_data.append(row)\n",
    "\n",
    "state_data_df = pd.DataFrame(state_data)\n",
    "\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'nws_data_state': state_data_df['nws_data_state'].isna().sum(),\n",
    "    'usgs_data_state': state_data_df['usgs_data_state'].isna().sum(),\n",
    "    'nws_preferred_state': state_data_df['nws_preferred_state'].isna().sum(),\n",
    "    'usgs_preferred_state': state_data_df['usgs_preferred_state'].isna().sum()}, index=[f'Number of NA Values in State Column, out of {len(state_data_df)} rows'])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "af1c18aa-cba4-472f-bc83-f497b8311960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lid</th>\n",
       "      <th>nws_data_state</th>\n",
       "      <th>usgs_data_state</th>\n",
       "      <th>nws_preferred_state</th>\n",
       "      <th>usgs_preferred_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00BRD</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00RDR</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1TEST</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>None</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AABDB</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AACLA</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>4674</td>\n",
       "      <td>RAPV2</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>None</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>4675</td>\n",
       "      <td>YWPC1</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>4676</td>\n",
       "      <td>WSPV2</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>None</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4677</th>\n",
       "      <td>4677</td>\n",
       "      <td>AMLC1</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>4678</td>\n",
       "      <td>AYMC1</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4679 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    lid nws_data_state usgs_data_state nws_preferred_state  \\\n",
       "0         0  00BRD     California            None          California   \n",
       "1         1  00RDR     California            None          California   \n",
       "2         2  1TEST        Vermont            None             Vermont   \n",
       "3         3  AABDB     California            None          California   \n",
       "4         4  AACLA     California            None          California   \n",
       "...     ...    ...            ...             ...                 ...   \n",
       "4674   4674  RAPV2       Virginia            None            Virginia   \n",
       "4675   4675  YWPC1     California            None          California   \n",
       "4676   4676  WSPV2       Virginia            None            Virginia   \n",
       "4677   4677  AMLC1     California            None          California   \n",
       "4678   4678  AYMC1     California            None          California   \n",
       "\n",
       "     usgs_preferred_state  \n",
       "0              California  \n",
       "1              California  \n",
       "2                 Vermont  \n",
       "3              California  \n",
       "4              California  \n",
       "...                   ...  \n",
       "4674             Virginia  \n",
       "4675           California  \n",
       "4676             Virginia  \n",
       "4677           California  \n",
       "4678           California  \n",
       "\n",
       "[4679 rows x 6 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "23741682-fc74-43d1-9da3-beb6c75cf31e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nws_data_state</th>\n",
       "      <th>usgs_data_state</th>\n",
       "      <th>nws_preferred_state</th>\n",
       "      <th>usgs_preferred_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of NA Values in State Column, out of 4679 rows</th>\n",
       "      <td>2</td>\n",
       "      <td>1041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    nws_data_state  \\\n",
       "Number of NA Values in State Column, out of 467...               2   \n",
       "\n",
       "                                                    usgs_data_state  \\\n",
       "Number of NA Values in State Column, out of 467...             1041   \n",
       "\n",
       "                                                    nws_preferred_state  \\\n",
       "Number of NA Values in State Column, out of 467...                    1   \n",
       "\n",
       "                                                    usgs_preferred_state  \n",
       "Number of NA Values in State Column, out of 467...                     1  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "007eb617-d224-46e4-ab8e-aab28679cf43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lid</th>\n",
       "      <th>nws_data_state</th>\n",
       "      <th>usgs_data_state</th>\n",
       "      <th>nws_preferred_state</th>\n",
       "      <th>usgs_preferred_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>302</td>\n",
       "      <td>BEAA3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    lid nws_data_state usgs_data_state nws_preferred_state  \\\n",
       "302    302  BEAA3           None            None                None   \n",
       "\n",
       "    usgs_preferred_state  \n",
       "302                 None  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noneval = 'None'\n",
    "\n",
    "# len(state_data_df[state_data_df['usgs_data_state']==None])\n",
    "\n",
    "# state_data_df[state_data_df['lid']=='YWPC1']\n",
    "\n",
    "# state_data_df[state_data_df['index']==2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# state_data_df[state_data_df['nws_data_state'].isna()]\n",
    "\n",
    "# state_data_df.loc[(state_data_df['nws_data_state'].isna()) & (state_data_df['usgs_data_state'].isna())]\n",
    "state_data_df.loc[(state_data_df['nws_preferred_state'].isna()) & (state_data_df['usgs_preferred_state'].isna())]\n",
    "\n",
    "\n",
    "\n",
    "## So now the question is, which of these 'state' columns are being used to pull the data? because if it's anything other than the USGS data state columnm, then we're actually ok I think...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69dadc-ac76-4029-a67a-4f3f60b5b038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e51c09-71af-4e43-9b02-ed9d0ae0842b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
