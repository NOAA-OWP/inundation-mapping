{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544d20bf-6273-4dd5-bae8-a42935b3e3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tools_shared_functions import (\n",
    "    aggregate_wbd_hucs,\n",
    "    filter_nwm_segments_by_stream_order,\n",
    "    flow_data,\n",
    "    get_metadata,\n",
    "    get_nwm_segs,\n",
    "    get_thresholds,\n",
    ")\n",
    "\n",
    "import utils.fim_logger as fl\n",
    "from utils.shared_variables import VIZ_PROJECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c7d9207-9b2b-431b-ab4e-3dbb3e2107f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Functions to get process and filter the metadata\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_of_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list = list_of_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "        lid_list.append(nws_lid)\n",
    "    if verbose == True:\n",
    "        print(f'List of LIDs: {lid_list}')\n",
    "        \n",
    "    return lid_list\n",
    "\n",
    "# lid_list = list_of_lids(conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_duplicate_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of duplicate LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    duplicate_lid_list = []\n",
    "     \n",
    "    \n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid in lid_list:\n",
    "            duplicate_lid_list.append(nws_lid)\n",
    "        else: \n",
    "            lid_list.append(nws_lid)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Length of unique LID list: {len(lid_list)}')\n",
    "        print(f'List of duplicate LIDs: {duplicate_lid_list}')\n",
    "\n",
    "        \n",
    "    return lid_list, duplicate_lid_list\n",
    "\n",
    "# lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_lid(lid_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by LID\n",
    "    \n",
    "    Example:\n",
    "    conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        lid = site['identifiers']['nws_lid']\n",
    "        if lid == lid_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'LID filter: {lid_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_state(state_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by state\n",
    "    \n",
    "    Example: \n",
    "    conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        state = site['nws_data']['state']\n",
    "        if state == state_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'State: {state_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dca1b28-976d-4487-9fd3-96253e1e83f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Testing get_metadata() functionality\n",
    "\n",
    "# --------- Inputs --------- \n",
    "\n",
    "search = 5\n",
    "\n",
    "nwm_us_search, nwm_ds_search = search, search\n",
    "\n",
    "\n",
    "# output_catfim_dir = \n",
    "API_BASE_URL = 'https://nwcal-wrds.nwc.nws.noaa.gov/api/location/v3.0'\n",
    "metadata_url = f'{API_BASE_URL}/metadata'\n",
    "\n",
    "\n",
    "# lid_to_run = \n",
    "# nwm_metafile = \n",
    "\n",
    "# --------- Code --------- \n",
    "\n",
    "all_meta_lists = []\n",
    "\n",
    "\n",
    "conus_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='nws_lid',\n",
    "    selector=['all'],\n",
    "    must_include='nws_data.rfc_forecast_point',\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "\n",
    "\n",
    "# Get metadata for Islands and Alaska\n",
    "islands_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='state',\n",
    "    selector=['HI', 'PR', 'AK'],\n",
    "    must_include=None,\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "# Append the lists\n",
    "all_meta_lists = conus_list + islands_list\n",
    "\n",
    "# print(islands_list)\n",
    "\n",
    "# with open(meta_file, \"wb\") as p_handle:\n",
    "#     pickle.dump(all_meta_lists, p_handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25a4979-2b0c-4f4f-ae41-6fec3768d39b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input metadata list length: 7631\n",
      "Output (unique) metadata list length: 7214\n",
      "Number of unique LIDs: 7214 \n",
      "Number of duplicate LIDs: 152 \n",
      "Number of None LIDs: 265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ New addition: filtering ------\n",
    "\n",
    "# -- function --\n",
    "def filter_metadata_list (metadata_list, verbose):\n",
    "    '''\n",
    "    \n",
    "    Filter metadata list to remove: \n",
    "    - sites where the nws_lid = None\n",
    "    - duplicate sites\n",
    "    \n",
    "    '''\n",
    "\n",
    "    unique_lids, duplicate_lids = [], []\n",
    "    duplicate_metadata_list, unique_metadata_list = [], []\n",
    "\n",
    "    nonelid_metadata_list = [] # TODO: remove eventually?    \n",
    "\n",
    "    for i, site in enumerate(metadata_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid == None:\n",
    "            # No LID available\n",
    "            nonelid_metadata_list.append(site)\n",
    "\n",
    "            # TODO: replace this with Continue, eventually we wont need this list\n",
    "\n",
    "        elif nws_lid in unique_lids:\n",
    "            # Duplicate LID\n",
    "            duplicate_lids.append(nws_lid)\n",
    "            duplicate_metadata_list.append(site)\n",
    "\n",
    "        else: \n",
    "            # Unique/unseen LID that's not None\n",
    "            unique_lids.append(nws_lid)\n",
    "            unique_metadata_list.append(site)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Input metadata list length: {len(metadata_list)}')\n",
    "        print(f'Output (unique) metadata list length: {len(unique_metadata_list)}')\n",
    "        print(f'Number of unique LIDs: {len(unique_lids)} \\nNumber of duplicate LIDs: {len(duplicate_lids)} \\nNumber of None LIDs: {len(nonelid_metadata_list)}')\n",
    "\n",
    "    return unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list # TODO: eventually, have it only return necessary objects\n",
    "\n",
    "\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(all_meta_lists, True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffc3b60-18de-4825-8766-80f3904fd6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Code: Single API call (only forecast points)\n",
      "State: Puerto Rico \n",
      "Number of sites: 5\n",
      "\n",
      "State: Hawaii \n",
      "Number of sites: 2\n",
      "\n",
      "State: Alaska \n",
      "Number of sites: 145\n",
      "\n",
      "\n",
      "Proposed Update: Double API call (forecast points + all HI, AK, and PR points)\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Puerto Rico \n",
      "Number of sites: 238\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Hawaii \n",
      "Number of sites: 495\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Alaska \n",
      "Number of sites: 1950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get state counts\n",
    "\n",
    "state_list = ['Puerto Rico', 'Hawaii', 'Alaska']\n",
    "\n",
    "print('Current Code: Single API call (only forecast points)')\n",
    "for state in state_list: \n",
    "    currentcode_state = filter_by_state(state, conus_list, True)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('Proposed Update: Double API call (forecast points + all HI, AK, and PR points)')\n",
    "print()\n",
    "for state in state_list: \n",
    "    # print('Before filtering out duplicates:')\n",
    "    # prefilt_state = filter_by_state(state, all_meta_lists, True)\n",
    "    print('AFTER filtering out duplicates:')\n",
    "    postfilt_state = filter_by_state(state, unique_metadata_list, True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2ad926-105c-4326-980c-3e4793b7b58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Connecticut \n",
      "Number of sites: 23\n",
      "State: New York \n",
      "Number of sites: 142\n",
      "State: Texas \n",
      "Number of sites: 380\n"
     ]
    }
   ],
   "source": [
    "postfilt_state = filter_by_state('Connecticut', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('New York', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('Texas', unique_metadata_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65996d68-b060-4cd3-b575-3e3d4172c532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input metadata list length: 4679\n",
      "Output (unique) metadata list length: 4679\n",
      "Number of unique LIDs: 4679 \n",
      "Number of duplicate LIDs: 0 \n",
      "Number of None LIDs: 0\n",
      "\n",
      "State: Alaska \n",
      "Number of sites: 145\n"
     ]
    }
   ],
   "source": [
    "## Current code formulation\n",
    "\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(conus_list, True)\n",
    "print()\n",
    "conus_list_filt = filter_by_state('Alaska', conus_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd1a64b-35c3-4f75-9e08-e936c17da1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LID filter: None \n",
      "Number of sites: 265\n"
     ]
    }
   ],
   "source": [
    "# lid_list, duplicate_lid_list = list_duplicate_lids(all_meta_lists, True)\n",
    "\n",
    "conus_list_filt = filter_by_lid(None, islands_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ec188992-fda6-4f8b-876f-be78c48c67cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# state_list, ___ = get_metadata(\n",
    "#     metadata_url,\n",
    "#     select_by='state',\n",
    "#     # selector=['HI', 'PR', 'AK'],\n",
    "#     selector=['AK'],\n",
    "\n",
    "#     # must_include='identifiers.nws_lid', ## ddin't work oh well\n",
    "#     must_include=None,\n",
    "\n",
    "#     # must_include='nws_data.rfc_forecast_point',\n",
    "#     upstream_trace_distance=nwm_us_search,\n",
    "#     downstream_trace_distance=nwm_ds_search,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba42000-3855-47eb-9f1c-f3c805f45b96",
   "metadata": {},
   "source": [
    "### Get a HUC list for a given HUC02 region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "034acbe3-d096-4dfd-a358-4ca409bfef64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19020101\n",
      "19020102\n",
      "19020103\n",
      "19020104\n",
      "19020201\n",
      "19020202\n",
      "19020203\n",
      "19020301\n",
      "19020302\n",
      "19020401\n",
      "19020402\n",
      "19020501\n",
      "19020502\n",
      "19020503\n",
      "19020504\n",
      "19020505\n",
      "19020601\n",
      "19020602\n",
      "19020800\n"
     ]
    }
   ],
   "source": [
    "fim_output_path = '/data/previous_fim/fim_4_5_2_11/'\n",
    "\n",
    "# huc2 = '20' # Hawaii\n",
    "# huc2 = '21' # Puerto Rico\n",
    "huc2 = '19' # Alaska\n",
    "\n",
    "all_hucs = os.listdir(fim_output_path)\n",
    "\n",
    "subsetted_hucs = [x for x in all_hucs if x.startswith(huc2)]\n",
    "\n",
    "for i in subsetted_hucs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a52847-3189-41b7-a755-3063512a96e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get stats for current CatFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca99d060-3963-497f-a8c0-cb117bbcd818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Inputs\n",
    "states = ['AK', 'PR', 'HI']\n",
    "\n",
    "## Previous runs\n",
    "catfim_folder_prev = '/data/catfim/'\n",
    "result_folders_prev = ['hand_4_5_11_1_flow_based', 'hand_4_5_11_1_stage_based', 'fim_4_5_2_11_flow_based', 'fim_4_5_2_11_stage_based']\n",
    "\n",
    "## Current test runs\n",
    "catfim_folder_testing = '/data/catfim/emily_test'\n",
    "result_folders_testing = ['site_filtering_HI_flow_based', 'site_filtering_HI_stage_based', \n",
    "                  'site_filtering_PR_flow_based', 'site_filtering_PR_stage_based', \n",
    "                  'site_filtering_AK_flow_based', 'site_filtering_AK_stage_based']\n",
    "\n",
    "def count_mapped_for_state(catfim_folder, result_folders, states):\n",
    "    # Read in CatFIM outputs\n",
    "    for result_folder in result_folders:\n",
    "        print()\n",
    "        print('-----' + result_folder + '-----')\n",
    "        \n",
    "        catfim_points_path = 'None'\n",
    "        \n",
    "        catfim_outputs_mapping_path = os.path.join(catfim_folder, result_folder, 'mapping')\n",
    "            \n",
    "        # Get filepath\n",
    "        for file in os.listdir(catfim_outputs_mapping_path):\n",
    "            if file.endswith('catfim_sites.csv'):\n",
    "                catfim_points_path = os.path.join(catfim_outputs_mapping_path, file)\n",
    "\n",
    "        if catfim_points_path == 'None':\n",
    "            print(f'No site csv found in {catfim_outputs_mapping_path}')\n",
    "            continue\n",
    "        \n",
    "        # Open points file\n",
    "        try:\n",
    "            catfim_points = gpd.read_file(catfim_points_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('An error occurred', e)\n",
    "            continue\n",
    "\n",
    "        # Get mapped vs unmapped data for the listed states\n",
    "        for state in states:\n",
    "            catfim_points_state = catfim_points[catfim_points['states'] == state]\n",
    "            \n",
    "            if len(catfim_points_state) != 0:\n",
    "                num_not_mapped = len(catfim_points_state[catfim_points_state['mapped'] == 'no'])\n",
    "                \n",
    "                catfim_points_state_mapped = catfim_points_state[catfim_points_state['mapped'] == 'yes']\n",
    "                num_mapped = len(catfim_points_state_mapped)\n",
    "                \n",
    "                huc_list = catfim_points_state['HUC8']\n",
    "                \n",
    "                if 'ahps_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['ahps_lid']\n",
    "                    lid_list_mapped = catfim_points_state_mapped['ahps_lid']\n",
    "                elif 'nws_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['nws_lid']\n",
    "                    lid_list_mapped = catfim_points_state_mapped['nws_lid']\n",
    "                else:\n",
    "                    print('Could not find ahps_lid or nws_lid column in csv.')                   \n",
    "                    print(catfim_points_state.columns)\n",
    "                    continue\n",
    "                \n",
    "                huc_list_unique = set(huc_list)\n",
    "                lid_list_unique = set(lid_list)\n",
    "                \n",
    "                num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "                num_duplicate_sites_mapped = len(lid_list_mapped) - len(set(lid_list_mapped))\n",
    "\n",
    "                print(f'{state} \\n   Mapped: {num_mapped} \\n   Not mapped: {num_not_mapped}')\n",
    "                \n",
    "                \n",
    "                print(f'   Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "                print(f'   Number of duplicate LIDs mapped: {num_duplicate_sites_mapped}')\n",
    "\n",
    "                print(f'   {len(huc_list_unique)} hucs: {huc_list_unique}')\n",
    "\n",
    "        # return catfim_points, catfim_points_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6255a914-3c44-4e02-8c20-a6f11eb5a12e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----hand_4_5_11_1_flow_based-----\n",
      "AK \n",
      "   Mapped: 14 \n",
      "   Not mapped: 39\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   14 hucs: {'19020202', '19020502', '19020201', '19020101', '19020504', '19020301', '19020402', '19020503', '19020104', '19020102', '19020505', '19020302', '19020401', '19020501'}\n",
      "PR \n",
      "   Mapped: 4 \n",
      "   Not mapped: 1\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'21010002', '21010005'}\n",
      "HI \n",
      "   Mapped: 1 \n",
      "   Not mapped: 1\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'20020000', '20010000'}\n",
      "\n",
      "-----hand_4_5_11_1_stage_based-----\n",
      "AK \n",
      "   Mapped: 13 \n",
      "   Not mapped: 40\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   14 hucs: {'19020202', '19020502', '19020201', '19020101', '19020504', '19020301', '19020402', '19020503', '19020104', '19020102', '19020505', '19020302', '19020401', '19020501'}\n",
      "PR \n",
      "   Mapped: 0 \n",
      "   Not mapped: 5\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'21010002', '21010005'}\n",
      "HI \n",
      "   Mapped: 0 \n",
      "   Not mapped: 2\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'20020000', '20010000'}\n",
      "\n",
      "-----fim_4_5_2_11_flow_based-----\n",
      "PR \n",
      "   Mapped: 68 \n",
      "   Not mapped: 177\n",
      "   Number of duplicate LIDs: 5\n",
      "   Number of duplicate LIDs mapped: 4\n",
      "   5 hucs: {'21010005', '21010002', '21010004', '21010003', '21010008'}\n",
      "HI \n",
      "   Mapped: 50 \n",
      "   Not mapped: 441\n",
      "   Number of duplicate LIDs: 2\n",
      "   Number of duplicate LIDs mapped: 2\n",
      "   7 hucs: {'20060000', '20020000', '20030000', '20050000', '20070000', '20010000', '20040000'}\n",
      "\n",
      "-----fim_4_5_2_11_stage_based-----\n",
      "PR \n",
      "   Mapped: 0 \n",
      "   Not mapped: 245\n",
      "   Number of duplicate LIDs: 5\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   5 hucs: {'21010005', '21010002', '21010004', '21010003', '21010008'}\n",
      "HI \n",
      "   Mapped: 0 \n",
      "   Not mapped: 491\n",
      "   Number of duplicate LIDs: 2\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   7 hucs: {'20060000', '20020000', '20030000', '20050000', '20070000', '20010000', '20040000'}\n"
     ]
    }
   ],
   "source": [
    "count_mapped_for_state(catfim_folder_prev, result_folders_prev, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f45696-893c-4709-9fe6-d1c223bd9020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----site_filtering_HI_flow_based-----\n",
      "HI \n",
      "   Mapped: 47 \n",
      "   Not mapped: 442\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   7 hucs: {'20060000', '20020000', '20030000', '20050000', '20070000', '20010000', '20040000'}\n",
      "\n",
      "-----site_filtering_HI_stage_based-----\n",
      "No site csv found in /data/catfim/emily_test/site_filtering_HI_stage_based/mapping\n",
      "\n",
      "-----site_filtering_PR_flow_based-----\n",
      "PR \n",
      "   Mapped: 59 \n",
      "   Not mapped: 181\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   5 hucs: {'21010005', '21010002', '21010004', '21010003', '21010008'}\n",
      "\n",
      "-----site_filtering_PR_stage_based-----\n",
      "No site csv found in /data/catfim/emily_test/site_filtering_PR_stage_based/mapping\n",
      "\n",
      "-----site_filtering_AK_flow_based-----\n",
      "AK \n",
      "   Mapped: 30 \n",
      "   Not mapped: 615\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   18 hucs: {'19020202', '19020502', '19020201', '19020101', '19020504', '19020601', '19020203', '19020301', '19020402', '19020503', '19020104', '19020800', '19020102', '19020505', '19020302', '19020602', '19020401', '19020501'}\n",
      "\n",
      "-----site_filtering_AK_stage_based-----\n",
      "AK \n",
      "   Mapped: 5 \n",
      "   Not mapped: 209\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   7 hucs: {'19020202', '19020203', '19020201', '19020101', '19020301', '19020104', '19020102'}\n"
     ]
    }
   ],
   "source": [
    "count_mapped_for_state(catfim_folder_testing, result_folders_testing, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "04988eab-2866-4d5b-882a-abb76e072df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate LIDs: 0\n",
      "7 hucs: {'20050000', '20060000', '20030000', '20020000', '20010000', '20070000', '20040000'}\n"
     ]
    }
   ],
   "source": [
    "huc_list = catfim_points_state['HUC8']\n",
    "lid_list = catfim_points_state['ahps_lid']\n",
    "\n",
    "huc_list_unique = set(huc_list)\n",
    "lid_list_unique = set(lid_list)\n",
    "num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "\n",
    "print(f'Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "print(f'{len(huc_list_unique)} hucs: {huc_list_unique}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae429a31-f83e-4594-9245-66cfb452b2be",
   "metadata": {},
   "source": [
    "### Test state column instablility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cce08d89-cf96-4905-a090-3ba3a09695b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_meta_list = conus_list # conus_list or islands_list\n",
    "\n",
    "# -----\n",
    "\n",
    "state_data = []\n",
    "\n",
    "for i, site in enumerate(input_meta_list):\n",
    "    lid = site['identifiers']['nws_lid']\n",
    "\n",
    "    nws_data_state = site['nws_data']['state']\n",
    "    usgs_data_state = site['usgs_data']['state']\n",
    "    nws_preferred_state = site['nws_preferred']['state']\n",
    "    usgs_preferred_state = site['usgs_preferred']['state']\n",
    "\n",
    "    row = {'index': i, 'lid': lid, \n",
    "           'nws_data_state':nws_data_state,\n",
    "           'usgs_data_state':usgs_data_state,\n",
    "           'nws_preferred_state':nws_preferred_state,\n",
    "           'usgs_preferred_state':usgs_preferred_state}\n",
    "\n",
    "    state_data.append(row)\n",
    "\n",
    "state_data_df = pd.DataFrame(state_data)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'nws_data_state': state_data_df['nws_data_state'].isna().sum(),\n",
    "    'usgs_data_state': state_data_df['usgs_data_state'].isna().sum(),\n",
    "    'nws_preferred_state': state_data_df['nws_preferred_state'].isna().sum(),\n",
    "    'usgs_preferred_state': state_data_df['usgs_preferred_state'].isna().sum()}, index=[f'Number of NA Values in State Column, out of {len(state_data_df)} rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "007eb617-d224-46e4-ab8e-aab28679cf43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lid</th>\n",
       "      <th>nws_data_state</th>\n",
       "      <th>usgs_data_state</th>\n",
       "      <th>nws_preferred_state</th>\n",
       "      <th>usgs_preferred_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>302</td>\n",
       "      <td>BEAA3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    lid nws_data_state usgs_data_state nws_preferred_state  \\\n",
       "302    302  BEAA3           None            None                None   \n",
       "\n",
       "    usgs_preferred_state  \n",
       "302                 None  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_data_df[state_data_df['nws_data_state'].isna()]\n",
    "# state_data_df.loc[(state_data_df['nws_data_state'].isna()) & (state_data_df['usgs_data_state'].isna())]\n",
    "state_data_df.loc[(state_data_df['nws_preferred_state'].isna()) & (state_data_df['usgs_preferred_state'].isna())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
