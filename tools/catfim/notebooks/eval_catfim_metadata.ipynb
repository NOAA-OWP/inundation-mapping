{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5de4a5-80cf-4cbf-97a9-1c8da52187b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "**README**\n",
    "\n",
    "This notebook is intended for developer and testing use, so the functions will \n",
    "probably not be as useful for outside users. That said, feel free to peruse the\n",
    "notebook and use what you wish from it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d20bf-6273-4dd5-bae8-a42935b3e3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tools_shared_functions import (\n",
    "    aggregate_wbd_hucs,\n",
    "    filter_nwm_segments_by_stream_order,\n",
    "    flow_data,\n",
    "    get_metadata,\n",
    "    get_nwm_segs,\n",
    "    get_thresholds,\n",
    ")\n",
    "\n",
    "import utils.fim_logger as fl\n",
    "from utils.shared_variables import VIZ_PROJECTION\n",
    "\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d9207-9b2b-431b-ab4e-3dbb3e2107f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Functions to retrieve, process, and filter the metadata\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_of_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list = list_of_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "        lid_list.append(nws_lid)\n",
    "    if verbose == True:\n",
    "        print(f'List of LIDs: {lid_list}')\n",
    "        \n",
    "    return lid_list\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_duplicate_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of duplicate LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    duplicate_lid_list = []\n",
    "     \n",
    "    \n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid in lid_list:\n",
    "            duplicate_lid_list.append(nws_lid)\n",
    "        else: \n",
    "            lid_list.append(nws_lid)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Length of unique LID list: {len(lid_list)}')\n",
    "        print(f'List of duplicate LIDs: {duplicate_lid_list}')\n",
    "\n",
    "        \n",
    "    return lid_list, duplicate_lid_list\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_lid(lid_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by LID\n",
    "    \n",
    "    Example:\n",
    "    conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        lid = site['identifiers']['nws_lid']\n",
    "        if lid == lid_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'LID filter: {lid_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_state(state_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by state\n",
    "    \n",
    "    Example: \n",
    "    conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        state = site['nws_data']['state']\n",
    "        if state == state_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'State: {state_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "print('Define functions complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca1b28-976d-4487-9fd3-96253e1e83f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing get_metadata() functionality\n",
    "\n",
    "# --------- Inputs --------- \n",
    "\n",
    "search = 5\n",
    "\n",
    "nwm_us_search, nwm_ds_search = search, search\n",
    "\n",
    "\n",
    "# output_catfim_dir = \n",
    "API_BASE_URL = 'https://nwcal-wrds.nwc.nws.noaa.gov/api/location/v3.0'\n",
    "metadata_url = f'{API_BASE_URL}/metadata'\n",
    "\n",
    "\n",
    "# lid_to_run = \n",
    "# nwm_metafile = \n",
    "\n",
    "# --------- Code --------- \n",
    "\n",
    "all_meta_lists = []\n",
    "\n",
    "\n",
    "conus_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='nws_lid',\n",
    "    selector=['all'],\n",
    "    must_include='nws_data.rfc_forecast_point',\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "\n",
    "\n",
    "# Get metadata for Islands and Alaska\n",
    "islands_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='state',\n",
    "    selector=['HI', 'PR', 'AK'],\n",
    "    must_include=None,\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "# Append the lists\n",
    "all_meta_lists = conus_list + islands_list\n",
    "\n",
    "# print(islands_list)\n",
    "\n",
    "# with open(meta_file, \"wb\") as p_handle:\n",
    "#     pickle.dump(all_meta_lists, p_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Metadata retrieval complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a4979-2b0c-4f4f-ae41-6fec3768d39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------ New addition: filtering ------\n",
    "\n",
    "# -- function --\n",
    "def filter_metadata_list (metadata_list, verbose):\n",
    "    '''\n",
    "    \n",
    "    Filter metadata list to remove: \n",
    "    - sites where the nws_lid = None\n",
    "    - duplicate sites\n",
    "    \n",
    "    '''\n",
    "\n",
    "    unique_lids, duplicate_lids = [], []\n",
    "    duplicate_metadata_list, unique_metadata_list = [], []\n",
    "\n",
    "    nonelid_metadata_list = [] # TODO: remove eventually?    \n",
    "\n",
    "    for i, site in enumerate(metadata_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid == None:\n",
    "            # No LID available\n",
    "            nonelid_metadata_list.append(site)\n",
    "\n",
    "            # TODO: replace this with Continue, eventually we wont need this list\n",
    "\n",
    "        elif nws_lid in unique_lids:\n",
    "            # Duplicate LID\n",
    "            duplicate_lids.append(nws_lid)\n",
    "            duplicate_metadata_list.append(site)\n",
    "\n",
    "        else: \n",
    "            # Unique/unseen LID that's not None\n",
    "            unique_lids.append(nws_lid)\n",
    "            unique_metadata_list.append(site)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Input metadata list length: {len(metadata_list)}')\n",
    "        print(f'Output (unique) metadata list length: {len(unique_metadata_list)}')\n",
    "        print(f'Number of unique LIDs: {len(unique_lids)} \\nNumber of duplicate LIDs: {len(duplicate_lids)} \\nNumber of None LIDs: {len(nonelid_metadata_list)}')\n",
    "\n",
    "    return unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list # TODO: eventually, have it only return necessary objects\n",
    "\n",
    "\n",
    "# Run filtering function\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(all_meta_lists, True)\n",
    "\n",
    "print('Filtering complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc3b60-18de-4825-8766-80f3904fd6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantify number of CatFIM sites available for each state (before and after filtering out duplicates)\n",
    "\n",
    "state_list = ['Puerto Rico', 'Hawaii', 'Alaska']\n",
    "\n",
    "print('Current Code: Single API call (only forecast points)')\n",
    "for state in state_list: \n",
    "    currentcode_state = filter_by_state(state, conus_list, True)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('Proposed Update: Double API call (forecast points + all HI, AK, and PR points)')\n",
    "print()\n",
    "for state in state_list: \n",
    "    # print('Before filtering out duplicates:')\n",
    "    # prefilt_state = filter_by_state(state, all_meta_lists, True)\n",
    "    print('AFTER filtering out duplicates:')\n",
    "    postfilt_state = filter_by_state(state, unique_metadata_list, True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ad926-105c-4326-980c-3e4793b7b58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "postfilt_state = filter_by_state('Connecticut', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('New York', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('Texas', unique_metadata_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65996d68-b060-4cd3-b575-3e3d4172c532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test current code formulation\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(conus_list, True)\n",
    "print()\n",
    "conus_list_filt = filter_by_state('Alaska', conus_list, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba42000-3855-47eb-9f1c-f3c805f45b96",
   "metadata": {},
   "source": [
    "### Get a HUC list given HUC02 region \n",
    "\n",
    "Creates a list of HUC8 IDs in the provided FIM output path based on an input HUC2 value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034acbe3-d096-4dfd-a358-4ca409bfef64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fim_output_path = '/data/previous_fim/fim_4_5_2_11/'\n",
    "\n",
    "# huc2 = '20' # Hawaii\n",
    "# huc2 = '21' # Puerto Rico\n",
    "huc2 = '19' # Alaska\n",
    "\n",
    "all_hucs = os.listdir(fim_output_path)\n",
    "\n",
    "subsetted_hucs = [x for x in all_hucs if x.startswith(huc2)]\n",
    "\n",
    "for i in subsetted_hucs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a52847-3189-41b7-a755-3063512a96e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get stats for CatFIM Results\n",
    "\n",
    "Summarizes the number of mapped and unmapped points for a list of CatFIM results given a list of states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99d060-3963-497f-a8c0-cb117bbcd818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function that counts mapped vs unmapped values for a CatFIM output folder\n",
    "\n",
    "def count_mapped_for_state(catfim_folder, result_folders, states):\n",
    "    # Read in CatFIM outputs\n",
    "    for result_folder in result_folders:\n",
    "        print()\n",
    "        print('-----' + result_folder + '-----')\n",
    "        \n",
    "        catfim_points_path = 'None'\n",
    "        catfim_outputs_mapping_path = os.path.join(catfim_folder, result_folder, 'mapping')\n",
    "            \n",
    "        # Get filepath\n",
    "        for file in os.listdir(catfim_outputs_mapping_path):\n",
    "            if file.endswith('catfim_sites.csv'):\n",
    "                catfim_points_path = os.path.join(catfim_outputs_mapping_path, file)\n",
    "\n",
    "        if catfim_points_path == 'None':\n",
    "            print(f'No site csv found in {catfim_outputs_mapping_path}')\n",
    "            continue\n",
    "        \n",
    "        # Open points file\n",
    "        try:\n",
    "            catfim_points = gpd.read_file(catfim_points_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('An error occurred', e)\n",
    "            continue\n",
    "\n",
    "        # Get mapped vs unmapped data for the listed states\n",
    "        for state in states:\n",
    "            catfim_points_state = catfim_points[catfim_points['states'] == state]\n",
    "            \n",
    "            if len(catfim_points_state) != 0:\n",
    "                num_not_mapped = len(catfim_points_state[catfim_points_state['mapped'] == 'no'])\n",
    "                \n",
    "                catfim_points_state_mapped = catfim_points_state[catfim_points_state['mapped'] == 'yes']\n",
    "                num_mapped = len(catfim_points_state_mapped)\n",
    "                \n",
    "                huc_list = catfim_points_state['HUC8']\n",
    "                \n",
    "                if 'ahps_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['ahps_lid']\n",
    "                    lid_list_mapped = catfim_points_state_mapped['ahps_lid']\n",
    "                elif 'nws_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['nws_lid']\n",
    "                    lid_list_mapped = catfim_points_state_mapped['nws_lid']\n",
    "                else:\n",
    "                    print('Could not find ahps_lid or nws_lid column in csv.')                   \n",
    "                    print(catfim_points_state.columns)\n",
    "                    continue\n",
    "                \n",
    "                huc_list_unique = set(huc_list)\n",
    "                lid_list_unique = set(lid_list)\n",
    "                \n",
    "                num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "                num_duplicate_sites_mapped = len(lid_list_mapped) - len(set(lid_list_mapped))\n",
    "\n",
    "                print(f'{state} \\n   Mapped: {num_mapped} \\n   Not mapped: {num_not_mapped}')\n",
    "                \n",
    "                \n",
    "                print(f'   Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "                print(f'   Number of duplicate LIDs mapped: {num_duplicate_sites_mapped}')\n",
    "\n",
    "                print(f'   {len(huc_list_unique)} hucs: {huc_list_unique}')\n",
    "\n",
    "        return catfim_points, #catfim_points_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808a93c-aee4-465a-8faa-18e941f5923a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up inputs for counting mapped points in the CatFIM results\n",
    "\n",
    "# Input state list\n",
    "states = ['AK', 'PR', 'HI']\n",
    "\n",
    "# Previous runs\n",
    "catfim_folder_prev = '/data/catfim/'\n",
    "result_folders_prev = ['hand_4_5_11_1_flow_based', 'hand_4_5_11_1_stage_based', 'fim_4_5_2_11_flow_based', 'fim_4_5_2_11_stage_based']\n",
    "\n",
    "# Current test runs\n",
    "catfim_folder_testing = '/data/catfim/emily_test'\n",
    "result_folders_testing = ['site_filtering_HI_flow_based', 'site_filtering_HI_stage_based', \n",
    "                  'site_filtering_PR_flow_based', 'site_filtering_PR_stage_based', \n",
    "                  'site_filtering_AK_flow_based', 'site_filtering_AK_stage_based']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255a914-3c44-4e02-8c20-a6f11eb5a12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run count_mapped_for_state() for previous CatFIM results\n",
    "\n",
    "count_mapped_for_state(catfim_folder_prev, result_folders_prev, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f45696-893c-4709-9fe6-d1c223bd9020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run count_mapped_for_state() for most recent CatFIM results\n",
    "\n",
    "count_mapped_for_state(catfim_folder_testing, result_folders_testing, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04988eab-2866-4d5b-882a-abb76e072df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quanitfy duplicate CatFIM sites\n",
    "\n",
    "huc_list = catfim_points_state['HUC8']\n",
    "lid_list = catfim_points_state['ahps_lid']\n",
    "\n",
    "huc_list_unique = set(huc_list)\n",
    "lid_list_unique = set(lid_list)\n",
    "num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "\n",
    "print(f'Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "print(f'{len(huc_list_unique)} hucs: {huc_list_unique}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae429a31-f83e-4594-9245-66cfb452b2be",
   "metadata": {},
   "source": [
    "### Create a Table of Sites with NA Value in the State Column\n",
    "\n",
    "This code is helpful for testing instability in metadata columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce08d89-cf96-4905-a090-3ba3a09695b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_meta_list = conus_list # conus_list or islands_list\n",
    "\n",
    "# -----\n",
    "\n",
    "state_data = []\n",
    "\n",
    "for i, site in enumerate(input_meta_list):\n",
    "    lid = site['identifiers']['nws_lid']\n",
    "\n",
    "    nws_data_state = site['nws_data']['state']\n",
    "    usgs_data_state = site['usgs_data']['state']\n",
    "    nws_preferred_state = site['nws_preferred']['state']\n",
    "    usgs_preferred_state = site['usgs_preferred']['state']\n",
    "\n",
    "    row = {'index': i, 'lid': lid, \n",
    "           'nws_data_state':nws_data_state,\n",
    "           'usgs_data_state':usgs_data_state,\n",
    "           'nws_preferred_state':nws_preferred_state,\n",
    "           'usgs_preferred_state':usgs_preferred_state}\n",
    "\n",
    "    state_data.append(row)s\n",
    "\n",
    "state_data_df = pd.DataFrame(state_data)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'nws_data_state': state_data_df['nws_data_state'].isna().sum(),\n",
    "    'usgs_data_state': state_data_df['usgs_data_state'].isna().sum(),\n",
    "    'nws_preferred_state': state_data_df['nws_preferred_state'].isna().sum(),\n",
    "    'usgs_preferred_state': state_data_df['usgs_preferred_state'].isna().sum()}, index=[f'Number of NA Values in State Column, out of {len(state_data_df)} rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007eb617-d224-46e4-ab8e-aab28679cf43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# state_data_df[state_data_df['nws_data_state'].isna()]\n",
    "# state_data_df.loc[(state_data_df['nws_data_state'].isna()) & (state_data_df['usgs_data_state'].isna())]\n",
    "state_data_df.loc[(state_data_df['nws_preferred_state'].isna()) & (state_data_df['usgs_preferred_state'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98a678-555e-47a9-bc64-87fd41883cda",
   "metadata": {},
   "source": [
    "### Parse CatFIM Logs into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323d4db-3cde-4029-9d06-f807a9419523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_path = '/data/catfim/emily_test/site_filtering_PR_stage_based/'\n",
    "log_file = 'catfim_2024_12_12-19_24_09.log'\n",
    "\n",
    "log_path = os.path.join(run_path, 'logs', log_file)\n",
    "\n",
    "\n",
    "out_df = []\n",
    "\n",
    "with open(log_path) as f:\n",
    "    for line in f:\n",
    "        # print(line)\n",
    "        \n",
    "        # Initialize variables\n",
    "        huc, lid, message_type, message = '', '', '', ''\n",
    "\n",
    "        # Get the HUC8\n",
    "        match = re.search(r\"\\d{8}\", line)\n",
    "        if match:\n",
    "            huc = match.group()\n",
    "            \n",
    "            # Get the LID\n",
    "            match2 = re.search(r\"(?<= : ).{5}\", line)\n",
    "            if match2:\n",
    "                lid = match2.group()\n",
    "                \n",
    "                if 'WARNING' in line:                    \n",
    "                    pattern = lid + ':'\n",
    "                    match3 = re.search(f\"(?<={pattern})(.*)\", line)\n",
    "                    if match3:\n",
    "                        message = match3.group()\n",
    "                        message_type = 'WARNING'\n",
    "\n",
    "                elif 'TRACE' in line: \n",
    "                    pattern = lid + ':'\n",
    "                    match3 = re.search(f\"(?<={pattern})(.*)\", line)\n",
    "                    if match3:\n",
    "                        message = match3.group()\n",
    "                        message_type = 'TRACE'\n",
    "                        \n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "                new_line = {'huc': huc, 'lid':lid, 'message_type':message_type, 'message':message}\n",
    "\n",
    "                out_df.append(new_line)\n",
    "                \n",
    "out_df = pd.DataFrame(out_df)\n",
    "                \n",
    "# out_df.to_csv('PR_error_logs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
