{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544d20bf-6273-4dd5-bae8-a42935b3e3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tools_shared_functions import (\n",
    "    aggregate_wbd_hucs,\n",
    "    filter_nwm_segments_by_stream_order,\n",
    "    flow_data,\n",
    "    get_metadata,\n",
    "    get_nwm_segs,\n",
    "    get_thresholds,\n",
    ")\n",
    "\n",
    "import utils.fim_logger as fl\n",
    "from utils.shared_variables import VIZ_PROJECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c7d9207-9b2b-431b-ab4e-3dbb3e2107f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Functions to get process and filter the metadata\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_of_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list = list_of_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "        lid_list.append(nws_lid)\n",
    "    if verbose == True:\n",
    "        print(f'List of LIDs: {lid_list}')\n",
    "        \n",
    "    return lid_list\n",
    "\n",
    "# lid_list = list_of_lids(conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def list_duplicate_lids(conus_list, verbose):\n",
    "    '''\n",
    "    Extract a list of duplicate LIDs from the conus_list\n",
    "    \n",
    "    Example: \n",
    "    lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "    '''\n",
    "    lid_list = []\n",
    "    duplicate_lid_list = []\n",
    "     \n",
    "    \n",
    "    for i, site in enumerate(conus_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid in lid_list:\n",
    "            duplicate_lid_list.append(nws_lid)\n",
    "        else: \n",
    "            lid_list.append(nws_lid)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Length of unique LID list: {len(lid_list)}')\n",
    "        print(f'List of duplicate LIDs: {duplicate_lid_list}')\n",
    "\n",
    "        \n",
    "    return lid_list, duplicate_lid_list\n",
    "\n",
    "# lid_list, duplicate_lid_list = list_duplicate_lids(conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_lid(lid_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by LID\n",
    "    \n",
    "    Example:\n",
    "    conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        lid = site['identifiers']['nws_lid']\n",
    "        if lid == lid_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'LID filter: {lid_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# conus_list_filt = filter_by_lid('None', conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def filter_by_state(state_filter, conus_list, verbose):\n",
    "    '''\n",
    "    Function to filter conus_list by state\n",
    "    \n",
    "    Example: \n",
    "    conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "    '''\n",
    "    conus_list_filt = []\n",
    "    for i, site in enumerate(conus_list):\n",
    "        state = site['nws_data']['state']\n",
    "        if state == state_filter:\n",
    "            conus_list_filt.append(site)\n",
    "    if verbose == True:\n",
    "        print(f'State: {state_filter} \\nNumber of sites: {len(conus_list_filt)}')\n",
    "        \n",
    "    return conus_list_filt\n",
    "\n",
    "# conus_list_filt = filter_by_state('Alaska', conus_list, True)\n",
    "\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fa68a-a711-4431-bf8d-7b20aeb93823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------- Inputs --------- \n",
    "\n",
    "search = 5\n",
    "\n",
    "nwm_us_search, nwm_ds_search = search, search\n",
    "\n",
    "\n",
    "# output_catfim_dir = \n",
    "API_BASE_URL = 'https://nwcal-wrds.nwc.nws.noaa.gov/api/location/v3.0'\n",
    "metadata_url = f'{API_BASE_URL}/metadata'\n",
    "\n",
    "# Get metadata for Islands and Alaska\n",
    "ak_test_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='state',\n",
    "    selector=['AK'],\n",
    "    must_include='nws_data.rfc_forecast_point',\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6591fb1-5591-4f0f-8f7f-d74fd9ad337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dca1b28-976d-4487-9fd3-96253e1e83f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Testing get_metadata() functionality\n",
    "\n",
    "# --------- Inputs --------- \n",
    "\n",
    "search = 5\n",
    "\n",
    "nwm_us_search, nwm_ds_search = search, search\n",
    "\n",
    "\n",
    "# output_catfim_dir = \n",
    "API_BASE_URL = 'https://nwcal-wrds.nwc.nws.noaa.gov/api/location/v3.0'\n",
    "metadata_url = f'{API_BASE_URL}/metadata'\n",
    "\n",
    "\n",
    "# lid_to_run = \n",
    "# nwm_metafile = \n",
    "\n",
    "# --------- Code --------- \n",
    "\n",
    "all_meta_lists = []\n",
    "\n",
    "\n",
    "conus_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='nws_lid',\n",
    "    selector=['all'],\n",
    "    must_include='nws_data.rfc_forecast_point',\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "\n",
    "\n",
    "# Get metadata for Islands and Alaska\n",
    "islands_list, ___ = get_metadata(\n",
    "    metadata_url,\n",
    "    select_by='state',\n",
    "    selector=['HI', 'PR', 'AK'],\n",
    "    must_include=None,\n",
    "    upstream_trace_distance=nwm_us_search,\n",
    "    downstream_trace_distance=nwm_ds_search,\n",
    ")\n",
    "# Append the lists\n",
    "all_meta_lists = conus_list + islands_list\n",
    "\n",
    "# print(islands_list)\n",
    "\n",
    "# with open(meta_file, \"wb\") as p_handle:\n",
    "#     pickle.dump(all_meta_lists, p_handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25a4979-2b0c-4f4f-ae41-6fec3768d39b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input metadata list length: 7631\n",
      "Output (unique) metadata list length: 7214\n",
      "Number of unique LIDs: 7214 \n",
      "Number of duplicate LIDs: 152 \n",
      "Number of None LIDs: 265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ New addition: filtering ------\n",
    "\n",
    "# -- function --\n",
    "def filter_metadata_list (metadata_list, verbose):\n",
    "    '''\n",
    "    \n",
    "    Filter metadata list to remove: \n",
    "    - sites where the nws_lid = None\n",
    "    - duplicate sites\n",
    "    \n",
    "    '''\n",
    "\n",
    "    unique_lids, duplicate_lids = [], []\n",
    "    duplicate_metadata_list, unique_metadata_list = [], []\n",
    "\n",
    "    nonelid_metadata_list = [] # TODO: remove eventually?    \n",
    "\n",
    "    for i, site in enumerate(metadata_list):\n",
    "        nws_lid = site['identifiers']['nws_lid']\n",
    "\n",
    "        if nws_lid == None:\n",
    "            # No LID available\n",
    "            nonelid_metadata_list.append(site)\n",
    "\n",
    "            # TODO: replace this with Continue, eventually we wont need this list\n",
    "\n",
    "        elif nws_lid in unique_lids:\n",
    "            # Duplicate LID\n",
    "            duplicate_lids.append(nws_lid)\n",
    "            duplicate_metadata_list.append(site)\n",
    "\n",
    "        else: \n",
    "            # Unique/unseen LID that's not None\n",
    "            unique_lids.append(nws_lid)\n",
    "            unique_metadata_list.append(site)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f'Input metadata list length: {len(metadata_list)}')\n",
    "        print(f'Output (unique) metadata list length: {len(unique_metadata_list)}')\n",
    "        print(f'Number of unique LIDs: {len(unique_lids)} \\nNumber of duplicate LIDs: {len(duplicate_lids)} \\nNumber of None LIDs: {len(nonelid_metadata_list)}')\n",
    "\n",
    "    return unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list # TODO: eventually, have it only return necessary objects\n",
    "\n",
    "\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(all_meta_lists, True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffc3b60-18de-4825-8766-80f3904fd6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Code: Single API call (only forecast points)\n",
      "State: Puerto Rico \n",
      "Number of sites: 5\n",
      "\n",
      "State: Hawaii \n",
      "Number of sites: 2\n",
      "\n",
      "State: Alaska \n",
      "Number of sites: 145\n",
      "\n",
      "\n",
      "Proposed Update: Double API call (forecast points + all HI, AK, and PR points)\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Puerto Rico \n",
      "Number of sites: 238\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Hawaii \n",
      "Number of sites: 495\n",
      "\n",
      "AFTER filtering out duplicates:\n",
      "State: Alaska \n",
      "Number of sites: 1950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get state counts\n",
    "\n",
    "state_list = ['Puerto Rico', 'Hawaii', 'Alaska']\n",
    "\n",
    "print('Current Code: Single API call (only forecast points)')\n",
    "for state in state_list: \n",
    "    currentcode_state = filter_by_state(state, conus_list, True)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('Proposed Update: Double API call (forecast points + all HI, AK, and PR points)')\n",
    "print()\n",
    "for state in state_list: \n",
    "    # print('Before filtering out duplicates:')\n",
    "    # prefilt_state = filter_by_state(state, all_meta_lists, True)\n",
    "    print('AFTER filtering out duplicates:')\n",
    "    postfilt_state = filter_by_state(state, unique_metadata_list, True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2ad926-105c-4326-980c-3e4793b7b58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Connecticut \n",
      "Number of sites: 23\n",
      "State: New York \n",
      "Number of sites: 142\n",
      "State: Texas \n",
      "Number of sites: 380\n"
     ]
    }
   ],
   "source": [
    "postfilt_state = filter_by_state('Connecticut', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('New York', unique_metadata_list, True)\n",
    "postfilt_state = filter_by_state('Texas', unique_metadata_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65996d68-b060-4cd3-b575-3e3d4172c532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input metadata list length: 4679\n",
      "Output (unique) metadata list length: 4679\n",
      "Number of unique LIDs: 4679 \n",
      "Number of duplicate LIDs: 0 \n",
      "Number of None LIDs: 0\n",
      "\n",
      "State: Alaska \n",
      "Number of sites: 145\n"
     ]
    }
   ],
   "source": [
    "## Current code formulation\n",
    "\n",
    "\n",
    "unique_lids, duplicate_lids, nonelid_metadata_list, duplicate_metadata_list, unique_metadata_list =  filter_metadata_list(conus_list, True)\n",
    "print()\n",
    "conus_list_filt = filter_by_state('Alaska', conus_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd1a64b-35c3-4f75-9e08-e936c17da1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LID filter: None \n",
      "Number of sites: 265\n"
     ]
    }
   ],
   "source": [
    "# lid_list, duplicate_lid_list = list_duplicate_lids(all_meta_lists, True)\n",
    "\n",
    "conus_list_filt = filter_by_lid(None, islands_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ec188992-fda6-4f8b-876f-be78c48c67cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# state_list, ___ = get_metadata(\n",
    "#     metadata_url,\n",
    "#     select_by='state',\n",
    "#     # selector=['HI', 'PR', 'AK'],\n",
    "#     selector=['AK'],\n",
    "\n",
    "#     # must_include='identifiers.nws_lid', ## ddin't work oh well\n",
    "#     must_include=None,\n",
    "\n",
    "#     # must_include='nws_data.rfc_forecast_point',\n",
    "#     upstream_trace_distance=nwm_us_search,\n",
    "#     downstream_trace_distance=nwm_ds_search,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba42000-3855-47eb-9f1c-f3c805f45b96",
   "metadata": {},
   "source": [
    "### Get a HUC list for a given HUC02 region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "034acbe3-d096-4dfd-a358-4ca409bfef64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19020101\n",
      "19020102\n",
      "19020103\n",
      "19020104\n",
      "19020201\n",
      "19020202\n",
      "19020203\n",
      "19020301\n",
      "19020302\n",
      "19020401\n",
      "19020402\n",
      "19020501\n",
      "19020502\n",
      "19020503\n",
      "19020504\n",
      "19020505\n",
      "19020601\n",
      "19020602\n",
      "19020800\n"
     ]
    }
   ],
   "source": [
    "fim_output_path = '/data/previous_fim/fim_4_5_2_11/'\n",
    "\n",
    "# huc2 = '20' # Hawaii\n",
    "# huc2 = '21' # Puerto Rico\n",
    "huc2 = '19' # Alaska\n",
    "\n",
    "all_hucs = os.listdir(fim_output_path)\n",
    "\n",
    "subsetted_hucs = [x for x in all_hucs if x.startswith(huc2)]\n",
    "\n",
    "for i in subsetted_hucs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a52847-3189-41b7-a755-3063512a96e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get stats for current CatFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca99d060-3963-497f-a8c0-cb117bbcd818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Inputs\n",
    "states = ['AK', 'PR', 'HI']\n",
    "\n",
    "## Previous runs\n",
    "catfim_folder_prev = '/data/catfim/'\n",
    "result_folders_prev = ['hand_4_5_11_1_flow_based', 'hand_4_5_11_1_stage_based', 'fim_4_5_2_11_flow_based', 'fim_4_5_2_11_stage_based']\n",
    "\n",
    "## Current test runs\n",
    "catfim_folder_testing = '/data/catfim/emily_test'\n",
    "result_folders_testing = ['site_filtering_HI_flow_based', 'site_filtering_HI_stage_based', \n",
    "                  'site_filtering_PR_flow_based', 'site_filtering_PR_stage_based', \n",
    "                  'site_filtering_AK_flow_based', 'site_filtering_AK_stage_based']\n",
    "\n",
    "def count_mapped_for_state(catfim_folder, result_folders, states):\n",
    "    # Read in CatFIM outputs\n",
    "    for result_folder in result_folders:\n",
    "        print()\n",
    "        print('-----' + result_folder + '-----')\n",
    "        \n",
    "        catfim_points_path = 'None'\n",
    "        \n",
    "        catfim_outputs_mapping_path = os.path.join(catfim_folder, result_folder, 'mapping')\n",
    "            \n",
    "        # Get filepath\n",
    "        for file in os.listdir(catfim_outputs_mapping_path):\n",
    "            if file.endswith('catfim_sites.csv'):\n",
    "                catfim_points_path = os.path.join(catfim_outputs_mapping_path, file)\n",
    "\n",
    "        if catfim_points_path == 'None':\n",
    "            print(f'No site csv found in {catfim_outputs_mapping_path}')\n",
    "            continue\n",
    "        \n",
    "        # Open points file\n",
    "        try:\n",
    "            catfim_points = gpd.read_file(catfim_points_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('An error occurred', e)\n",
    "            continue\n",
    "\n",
    "        # Get mapped vs unmapped data for the listed states\n",
    "        for state in states:\n",
    "            catfim_points_state = catfim_points[catfim_points['states'] == state]\n",
    "            \n",
    "            if len(catfim_points_state) != 0:\n",
    "                num_not_mapped = len(catfim_points_state[catfim_points_state['mapped'] == 'no'])\n",
    "                \n",
    "                catfim_points_state_mapped = catfim_points_state[catfim_points_state['mapped'] == 'yes']\n",
    "                num_mapped = len(catfim_points_state_mapped)\n",
    "                \n",
    "                huc_list = catfim_points_state['HUC8']\n",
    "                \n",
    "                if 'ahps_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['ahps_lid']\n",
    "                    lid_list_mapped = catfim_points_state_mapped['ahps_lid']\n",
    "                elif 'nws_lid' in catfim_points_state.columns:\n",
    "                    lid_list = catfim_points_state['nws_lid']\n",
    "                    lid_list_mapped = catfim_points_state_mapped['nws_lid']\n",
    "                else:\n",
    "                    print('Could not find ahps_lid or nws_lid column in csv.')                   \n",
    "                    print(catfim_points_state.columns)\n",
    "                    continue\n",
    "                \n",
    "                huc_list_unique = set(huc_list)\n",
    "                lid_list_unique = set(lid_list)\n",
    "                \n",
    "                num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "                num_duplicate_sites_mapped = len(lid_list_mapped) - len(set(lid_list_mapped))\n",
    "\n",
    "                print(f'{state} \\n   Mapped: {num_mapped} \\n   Not mapped: {num_not_mapped}')\n",
    "                \n",
    "                \n",
    "                print(f'   Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "                print(f'   Number of duplicate LIDs mapped: {num_duplicate_sites_mapped}')\n",
    "\n",
    "                print(f'   {len(huc_list_unique)} hucs: {huc_list_unique}')\n",
    "\n",
    "        return catfim_points, #catfim_points_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9808a93c-aee4-465a-8faa-18e941f5923a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# catfim_points_pr = count_mapped_for_state('/data/catfim/emily_test/', ['site_filtering_PR_stage_based'], 'PR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6255a914-3c44-4e02-8c20-a6f11eb5a12e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----hand_4_5_11_1_flow_based-----\n",
      "AK \n",
      "   Mapped: 14 \n",
      "   Not mapped: 39\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   14 hucs: {'19020202', '19020502', '19020201', '19020101', '19020504', '19020301', '19020402', '19020503', '19020104', '19020102', '19020505', '19020302', '19020401', '19020501'}\n",
      "PR \n",
      "   Mapped: 4 \n",
      "   Not mapped: 1\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'21010002', '21010005'}\n",
      "HI \n",
      "   Mapped: 1 \n",
      "   Not mapped: 1\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'20020000', '20010000'}\n",
      "\n",
      "-----hand_4_5_11_1_stage_based-----\n",
      "AK \n",
      "   Mapped: 13 \n",
      "   Not mapped: 40\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   14 hucs: {'19020202', '19020502', '19020201', '19020101', '19020504', '19020301', '19020402', '19020503', '19020104', '19020102', '19020505', '19020302', '19020401', '19020501'}\n",
      "PR \n",
      "   Mapped: 0 \n",
      "   Not mapped: 5\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'21010002', '21010005'}\n",
      "HI \n",
      "   Mapped: 0 \n",
      "   Not mapped: 2\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   2 hucs: {'20020000', '20010000'}\n",
      "\n",
      "-----fim_4_5_2_11_flow_based-----\n",
      "PR \n",
      "   Mapped: 68 \n",
      "   Not mapped: 177\n",
      "   Number of duplicate LIDs: 5\n",
      "   Number of duplicate LIDs mapped: 4\n",
      "   5 hucs: {'21010005', '21010002', '21010004', '21010003', '21010008'}\n",
      "HI \n",
      "   Mapped: 50 \n",
      "   Not mapped: 441\n",
      "   Number of duplicate LIDs: 2\n",
      "   Number of duplicate LIDs mapped: 2\n",
      "   7 hucs: {'20060000', '20020000', '20030000', '20050000', '20070000', '20010000', '20040000'}\n",
      "\n",
      "-----fim_4_5_2_11_stage_based-----\n",
      "PR \n",
      "   Mapped: 0 \n",
      "   Not mapped: 245\n",
      "   Number of duplicate LIDs: 5\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   5 hucs: {'21010005', '21010002', '21010004', '21010003', '21010008'}\n",
      "HI \n",
      "   Mapped: 0 \n",
      "   Not mapped: 491\n",
      "   Number of duplicate LIDs: 2\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   7 hucs: {'20060000', '20020000', '20030000', '20050000', '20070000', '20010000', '20040000'}\n"
     ]
    }
   ],
   "source": [
    "count_mapped_for_state(catfim_folder_prev, result_folders_prev, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f45696-893c-4709-9fe6-d1c223bd9020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----site_filtering_HI_flow_based-----\n",
      "HI \n",
      "   Mapped: 47 \n",
      "   Not mapped: 442\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   7 hucs: {'20060000', '20020000', '20030000', '20050000', '20070000', '20010000', '20040000'}\n",
      "\n",
      "-----site_filtering_HI_stage_based-----\n",
      "No site csv found in /data/catfim/emily_test/site_filtering_HI_stage_based/mapping\n",
      "\n",
      "-----site_filtering_PR_flow_based-----\n",
      "PR \n",
      "   Mapped: 59 \n",
      "   Not mapped: 181\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   5 hucs: {'21010005', '21010002', '21010004', '21010003', '21010008'}\n",
      "\n",
      "-----site_filtering_PR_stage_based-----\n",
      "No site csv found in /data/catfim/emily_test/site_filtering_PR_stage_based/mapping\n",
      "\n",
      "-----site_filtering_AK_flow_based-----\n",
      "AK \n",
      "   Mapped: 30 \n",
      "   Not mapped: 615\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   18 hucs: {'19020202', '19020502', '19020201', '19020101', '19020504', '19020601', '19020203', '19020301', '19020402', '19020503', '19020104', '19020800', '19020102', '19020505', '19020302', '19020602', '19020401', '19020501'}\n",
      "\n",
      "-----site_filtering_AK_stage_based-----\n",
      "AK \n",
      "   Mapped: 5 \n",
      "   Not mapped: 209\n",
      "   Number of duplicate LIDs: 0\n",
      "   Number of duplicate LIDs mapped: 0\n",
      "   7 hucs: {'19020202', '19020203', '19020201', '19020101', '19020301', '19020104', '19020102'}\n"
     ]
    }
   ],
   "source": [
    "count_mapped_for_state(catfim_folder_testing, result_folders_testing, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "04988eab-2866-4d5b-882a-abb76e072df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate LIDs: 0\n",
      "7 hucs: {'20050000', '20060000', '20030000', '20020000', '20010000', '20070000', '20040000'}\n"
     ]
    }
   ],
   "source": [
    "huc_list = catfim_points_state['HUC8']\n",
    "lid_list = catfim_points_state['ahps_lid']\n",
    "\n",
    "huc_list_unique = set(huc_list)\n",
    "lid_list_unique = set(lid_list)\n",
    "num_duplicate_sites = len(lid_list) - len(lid_list_unique)\n",
    "\n",
    "print(f'Number of duplicate LIDs: {num_duplicate_sites}')\n",
    "print(f'{len(huc_list_unique)} hucs: {huc_list_unique}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae429a31-f83e-4594-9245-66cfb452b2be",
   "metadata": {},
   "source": [
    "### Test state column instablility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cce08d89-cf96-4905-a090-3ba3a09695b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_meta_list = conus_list # conus_list or islands_list\n",
    "\n",
    "# -----\n",
    "\n",
    "state_data = []\n",
    "\n",
    "for i, site in enumerate(input_meta_list):\n",
    "    lid = site['identifiers']['nws_lid']\n",
    "\n",
    "    nws_data_state = site['nws_data']['state']\n",
    "    usgs_data_state = site['usgs_data']['state']\n",
    "    nws_preferred_state = site['nws_preferred']['state']\n",
    "    usgs_preferred_state = site['usgs_preferred']['state']\n",
    "\n",
    "    row = {'index': i, 'lid': lid, \n",
    "           'nws_data_state':nws_data_state,\n",
    "           'usgs_data_state':usgs_data_state,\n",
    "           'nws_preferred_state':nws_preferred_state,\n",
    "           'usgs_preferred_state':usgs_preferred_state}\n",
    "\n",
    "    state_data.append(row)\n",
    "\n",
    "state_data_df = pd.DataFrame(state_data)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'nws_data_state': state_data_df['nws_data_state'].isna().sum(),\n",
    "    'usgs_data_state': state_data_df['usgs_data_state'].isna().sum(),\n",
    "    'nws_preferred_state': state_data_df['nws_preferred_state'].isna().sum(),\n",
    "    'usgs_preferred_state': state_data_df['usgs_preferred_state'].isna().sum()}, index=[f'Number of NA Values in State Column, out of {len(state_data_df)} rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "007eb617-d224-46e4-ab8e-aab28679cf43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lid</th>\n",
       "      <th>nws_data_state</th>\n",
       "      <th>usgs_data_state</th>\n",
       "      <th>nws_preferred_state</th>\n",
       "      <th>usgs_preferred_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>302</td>\n",
       "      <td>BEAA3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    lid nws_data_state usgs_data_state nws_preferred_state  \\\n",
       "302    302  BEAA3           None            None                None   \n",
       "\n",
       "    usgs_preferred_state  \n",
       "302                 None  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_data_df[state_data_df['nws_data_state'].isna()]\n",
    "# state_data_df.loc[(state_data_df['nws_data_state'].isna()) & (state_data_df['usgs_data_state'].isna())]\n",
    "state_data_df.loc[(state_data_df['nws_preferred_state'].isna()) & (state_data_df['usgs_preferred_state'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98a678-555e-47a9-bc64-87fd41883cda",
   "metadata": {},
   "source": [
    "### Review CatFIM Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe1c3fd-4b84-4574-b9bf-e89ec5bda6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_path = '/data/catfim/emily_test/site_filtering_PR_stage_based/'\n",
    "log_file = 'catfim_2024_12_12-19_24_09.log'\n",
    "\n",
    "log_path = os.path.join(run_path, 'logs', log_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8323d4db-3cde-4029-9d06-f807a9419523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "out_df = []\n",
    "\n",
    "with open(log_path) as f:\n",
    "    for line in f:\n",
    "        # print(line)\n",
    "        \n",
    "        # Initialize variables\n",
    "        huc, lid, message_type, message = '', '', '', ''\n",
    "\n",
    "        # Get the HUC8\n",
    "        match = re.search(r\"\\d{8}\", line)\n",
    "\n",
    "        if match:\n",
    "            huc = match.group()\n",
    "            \n",
    "            # Get the LID\n",
    "            match2 = re.search(r\"(?<= : ).{5}\", line)\n",
    "            if match2:\n",
    "                lid = match2.group()\n",
    "                \n",
    "                if 'WARNING' in line:\n",
    "            \n",
    "                    # print(line)\n",
    "                    \n",
    "                    # Get the text\n",
    "                    pattern = lid+':'\n",
    "                    match3 = re.search(f\"(?<={pattern})(.*)\", line)\n",
    "                    if match3:\n",
    "                        message = match3.group()\n",
    "                        message_type = 'WARNING'\n",
    "\n",
    "                elif 'TRACE' in line: \n",
    "                    # Get the text\n",
    "                    pattern = lid+':'\n",
    "                    match3 = re.search(f\"(?<={pattern})(.*)\", line)\n",
    "                    if match3:\n",
    "                        message = match3.group()\n",
    "                        message_type = 'TRACE'\n",
    "                        \n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "                new_line = {'huc': huc, 'lid':lid, 'message_type':message_type, 'message':message}\n",
    "\n",
    "                out_df.append(new_line)\n",
    "                        \n",
    "                        \n",
    "                \n",
    "out_df = pd.DataFrame(out_df)\n",
    "\n",
    "                \n",
    "# out_df.to_csv('PR_error_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7b763ad-00eb-43f2-944a-2fceb2b96e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## __create_acceptable_usgs_elev_df from generate_categorical_fim.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from catfim.generate_categorical_fim_flows import generate_flows\n",
    "from catfim.generate_categorical_fim_mapping import (\n",
    "    manage_catfim_mapping,\n",
    "    post_process_cat_fim_for_viz,\n",
    "    produce_stage_based_lid_tifs,\n",
    ")\n",
    "from tools_shared_functions import (\n",
    "    filter_nwm_segments_by_stream_order,\n",
    "    get_datum,\n",
    "    get_nwm_segs,\n",
    "    get_thresholds,\n",
    "    ngvd_to_navd_ft,\n",
    ")\n",
    "from tools_shared_variables import (\n",
    "    acceptable_alt_acc_thresh,\n",
    "    acceptable_alt_meth_code_list,\n",
    "    acceptable_coord_acc_code_list,\n",
    "    acceptable_coord_method_code_list,\n",
    "    acceptable_site_type_list,\n",
    ")\n",
    "\n",
    "import utils.fim_logger as fl\n",
    "from utils.shared_variables import VIZ_PROJECTION\n",
    "\n",
    "\n",
    "# from itertools import repeat\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# global RLOG\n",
    "FLOG = fl.FIM_logger()  # the non mp version\n",
    "MP_LOG = fl.FIM_logger()  # the Multi Proc version\n",
    "\n",
    "gpd.options.io_engine = \"pyogrio\"\n",
    "\n",
    "def __create_acceptable_usgs_elev_df(usgs_elev_df, huc_lid_id):\n",
    "    acceptable_usgs_elev_df = None\n",
    "    try:\n",
    "        # Drop columns that offend acceptance criteria\n",
    "        print(len(usgs_elev_df)) ## TEMP DEBUG\n",
    "        usgs_elev_df['acceptable_codes'] = (\n",
    "            # usgs_elev_df['usgs_data_coord_accuracy_code'].isin(acceptable_coord_acc_code_list)\n",
    "            # & usgs_elev_df['usgs_data_coord_method_code'].isin(acceptable_coord_method_code_list)\n",
    "            usgs_elev_df['usgs_data_alt_method_code'].isin(acceptable_alt_meth_code_list)\n",
    "            & usgs_elev_df['usgs_data_site_type'].isin(acceptable_site_type_list)\n",
    "        )\n",
    "        \n",
    "        print(len(usgs_elev_df)) ## TEMP DEBUG\n",
    "        \n",
    "        usgs_elev_df = usgs_elev_df.astype({'usgs_data_alt_accuracy_code': float})\n",
    "        usgs_elev_df['acceptable_alt_error'] = np.where(\n",
    "            usgs_elev_df['usgs_data_alt_accuracy_code'] <= acceptable_alt_acc_thresh, True, False\n",
    "        )\n",
    "        \n",
    "        print('after \n",
    "        print(len(usgs_elev_df)) ## TEMP DEBUG\n",
    "              \n",
    "        acceptable_usgs_elev_df = usgs_elev_df[\n",
    "            (usgs_elev_df['acceptable_codes'] == True) & (usgs_elev_df['acceptable_alt_error'] == True)\n",
    "        ]\n",
    "\n",
    "        # # TEMP DEBUG Record row difference and write it to a CSV or something\n",
    "        # label = 'Old code' ## TEMP DEBUG\n",
    "        # num_potential_rows = usgs_elev_df.shape[0]\n",
    "        # num_acceptable_rows = acceptable_usgs_elev_df.shape[0]\n",
    "        # out_message = f'{label}: kept {num_acceptable_rows} rows out of {num_potential_rows} available rows.'\n",
    "\n",
    "    except Exception:\n",
    "        # Not sure any of the sites actually have those USGS-related\n",
    "        # columns in this particular file, so just assume it's fine to use\n",
    "\n",
    "        # print(\"(Various columns related to USGS probably not in this csv)\")\n",
    "        # print(f\"Exception: \\n {repr(e)} \\n\")\n",
    "        MP_LOG.error(f\"{huc_lid_id}: An error has occurred while working with the usgs_elev table\")\n",
    "        MP_LOG.error(traceback.format_exc())\n",
    "        acceptable_usgs_elev_df = usgs_elev_df\n",
    "\n",
    "    return acceptable_usgs_elev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "560c9420-b0c0-40c2-895b-6546bbb51365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>nws_lid</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>HydroID</th>\n",
       "      <th>levpa_id</th>\n",
       "      <th>dem_elevation</th>\n",
       "      <th>dem_adj_elevation</th>\n",
       "      <th>order_</th>\n",
       "      <th>LakeID</th>\n",
       "      <th>HUC8</th>\n",
       "      <th>...</th>\n",
       "      <th>mainstem</th>\n",
       "      <th>acceptable_codes</th>\n",
       "      <th>acceptable_alt_error</th>\n",
       "      <th>source</th>\n",
       "      <th>stream_stn</th>\n",
       "      <th>fid_xs</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>geometry_ln</th>\n",
       "      <th>geometry_snapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [location_id, nws_lid, feature_id, HydroID, levpa_id, dem_elevation, dem_adj_elevation, order_, LakeID, HUC8, snap_distance, wrds_timestamp, nrldb_timestamp, nwis_timestamp, metadata_sources, goes_id, env_can_gage_id, nws_data_name, nws_data_wfo, nws_data_rfc, nws_data_geo_rfc, nws_data_latitude, nws_data_longitude, nws_data_map_link, nws_data_horizontal_datum_name, nws_data_state, nws_data_county, nws_data_county_code, nws_data_huc, nws_data_hsa, nws_data_zero_datum, nws_data_vertical_datum_name, nws_data_rfc_forecast_point, nws_data_rfc_defined_fcst_point, nws_data_riverpoint, usgs_data_name, usgs_data_geo_rfc, usgs_data_latitude, usgs_data_longitude, usgs_data_map_link, usgs_data_coord_accuracy_code, usgs_data_latlon_datum_name, usgs_data_coord_method_code, usgs_data_state, usgs_data_huc, usgs_data_site_type, usgs_data_altitude, usgs_data_alt_accuracy_code, usgs_data_alt_datum_code, usgs_data_alt_method_code, usgs_data_drainage_area, usgs_data_drainage_area_units, usgs_data_contrib_drainage_area, usgs_data_active, usgs_data_gages_ii_reference, nwm_feature_data_downstream_feature_id, nwm_feature_data_latitude, nwm_feature_data_longitude, nwm_feature_data_altitude, nwm_feature_data_stream_length, nwm_feature_data_stream_order, nwm_feature_data_mannings_roughness, nwm_feature_data_slope, nwm_feature_data_channel_side_slope, nwm_feature_data_nhd_waterbody_comid, env_can_gage_data_name, env_can_gage_data_latitude, env_can_gage_data_longitude, env_can_gage_data_map_link, env_can_gage_data_drainage_area, env_can_gage_data_contrib_drainage_area, env_can_gage_data_water_course, nws_preferred_name, nws_preferred_latitude, nws_preferred_longitude, nws_preferred_latlon_datum_name, nws_preferred_state, nws_preferred_huc, usgs_preferred_name, usgs_preferred_latitude, usgs_preferred_longitude, usgs_preferred_latlon_datum_name, usgs_preferred_state, usgs_preferred_huc, crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id, crosswalk_datasets_location_nwm_crosswalk_dataset_name, crosswalk_datasets_location_nwm_crosswalk_dataset_description, crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id, crosswalk_datasets_nws_usgs_crosswalk_dataset_name, crosswalk_datasets_nws_usgs_crosswalk_dataset_description, assigned_crs, name, states, curve, mainstem, acceptable_codes, acceptable_alt_error, source, stream_stn, fid_xs, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 104 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usgs_elev_table = '/data/previous_fim/fim_4_5_2_11/21010005/usgs_elev_table.csv'\n",
    "huc_lid_id = 'bzap4'\n",
    "\n",
    "\n",
    "usgs_elev_df = pd.read_csv(usgs_elev_table)\n",
    "\n",
    "\n",
    "\n",
    "acceptable_usgs_elev_df = __create_acceptable_usgs_elev_df(usgs_elev_df, huc_lid_id)\n",
    "\n",
    "acceptable_usgs_elev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b8d4fc4-0733-4eca-952f-60f53305a7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(out_df['lid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6cce11a-27b7-4302-b0b7-ba304fa8087c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# message_string = 'Unable to find gage data'\n",
    "message_string = 'stage values'\n",
    "\n",
    "stage_val_lids = out_df[out_df['message'].str.contains('stage values')]['lid']\n",
    "no_gage_data_lids = out_df[out_df['message'].str.contains('Unable to find gage data')]['lid']\n",
    "\n",
    "\n",
    "\n",
    "# # Using set intersection\n",
    "lids_stageval_nogagedata = list(set(stage_val_lids) & set(no_gage_data_lids))\n",
    "\n",
    "# len(stage_val_lids)\n",
    "# len(no_gage_data_lids)\n",
    "# len(lids_stageval_nogagedata)\n",
    "\n",
    "# print(stage_val_lids)\n",
    "# lids_stageval_nogagedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d3712-c9ae-45d5-9fd5-e49ca29e194b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c6bb5-c24b-4de6-8645-d2aabd61be26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
